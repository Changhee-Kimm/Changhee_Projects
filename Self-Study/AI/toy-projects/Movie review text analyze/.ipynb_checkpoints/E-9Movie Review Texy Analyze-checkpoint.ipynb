{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비와 확인\n",
    "\n",
    "## 2. 데이터로더 구성\n",
    "\n",
    "## 3. 모델구성을 위한 데이터 분석 및 가공\n",
    " - 데이터셋 내 문장 길이 분포\n",
    " - 적절한 최대 문장 길이 지정\n",
    " - keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가\n",
    " \n",
    " \n",
    "## 4. 모델구성 및 validation set 구성\n",
    " - 3가지 이상\n",
    " \n",
    "## 5. 모델 훈련\n",
    "\n",
    "## 6. Loss, Accuracy 그래프 시각화\n",
    "\n",
    "## 7. 학습된 Embedding 레이어 분석\n",
    "\n",
    "## 8 . 한국어 Word2Vec 임베딩 활용하여 성능개선\n",
    "\n",
    " - 한국어 Word2Vec은 다음 경로에서 구할 수 있습니다.\n",
    "\n",
    "https://github.com/Kyubyong/wordvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비와 확인\n",
    "```\n",
    "$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
    "$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
    "$ mv ratings_*.txt ~/aiffel/sentimental_classification\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로더 구성\n",
    "\n",
    " - 데이터의 중복 제거\n",
    " - NaN 결측치 제거\n",
    " - 한국어 토크나이저로 토큰화\n",
    " - 불용어(Stopwords) 제거\n",
    " - 사전word_to_index 구성\n",
    " - 텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
    " - X_train, y_train, X_test, y_test, word_to_index 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도',\n",
    "             '를','으로','자','에','와','한','하다','다','고','하','을','.','..',\n",
    "             ',','었','만','는데','로','음','것','아','네요','어','같','했','에서','기','네','거'\n",
    "             ,'수','되','면','게','지','있','나','점','인','주','내','~','던','어요','할','겠','1'\n",
    "             ,'해','습니다','...','더','라','그','볼']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "###상위 100개 에서 불용어 제거하기\n",
    "for i in range(50):\n",
    "    print(index_to_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델구성을 위한 데이터 분석 및 가공\n",
    "\n",
    "- 데이터셋 내 문장 길이 분포\n",
    "- 적절한 최대 문장 길이 지정\n",
    "- keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 4 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pre padding이 더 성능이 좋다.\n",
    "#### RNN 은 뒤로 갈 수록 앞의 데이터가 쌓이게 되는데, 최종 state값에 가장 영향을 많이 미친다.\n",
    "#### 뒤의 데이터가 0이면 끝에 가서 데이터의 의미가 더 줄어들게 된다.\n",
    "#### 따라서 앞에 부분을 0으로 Padding하는것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델구성 및 validation set 구성\n",
    " - 3가지 이상\n",
    " \n",
    " \n",
    " #### 4-1. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 47  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "\n",
    "model_LSTM = keras.Sequential()\n",
    "# [[YOUR CODE]]\n",
    "model_LSTM.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_LSTM.add(keras.layers.LSTM(128))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model_LSTM.add(keras.layers.Dense(8, activation='relu'))\n",
    "model_LSTM.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2.   1-D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1DCNN = keras.Sequential()\n",
    "model_1DCNN.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_1DCNN.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model_1DCNN.add(keras.layers.MaxPooling1D(5))\n",
    "model_1DCNN.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model_1DCNN.add(keras.layers.GlobalMaxPooling1D())\n",
    "model_1DCNN.add(keras.layers.Dense(8, activation='relu'))\n",
    "model_1DCNN.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model_1DCNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3.  GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GlobMaxPool = keras.Sequential()\n",
    "model_GlobMaxPool.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_GlobMaxPool.add(keras.layers.GlobalMaxPooling1D())\n",
    "model_GlobMaxPool.add(keras.layers.Dense(8, activation='relu'))\n",
    "model_GlobMaxPool.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model_GlobMaxPool.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set 46182건 분리\n",
    "X_val = X_train[:46182]   \n",
    "y_val = y_train[:46182]\n",
    "\n",
    "# validation set을 제외한 나머지 100000건\n",
    "partial_X_train = X_train[46182:]  \n",
    "partial_y_train = y_train[46182:]\n",
    "\n",
    "print(partial_X_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_LSTM.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_LSTM = model_LSTM.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1DCNN.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=8  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_1DCNN = model_1DCNN.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GlobMaxPool.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=7  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_GlobMaxPool = model_GlobMaxPool.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss, Accuracy 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_LSTM = model_LSTM.evaluate(X_test,  y_test, verbose=2)\n",
    "results_1DCNN = model_1DCNN.evaluate(X_test,  y_test, verbose=2)\n",
    "results_GlobMaxPool = model_GlobMaxPool.evaluate(X_test,  y_test, verbose=2)\n",
    "print(results_LSTM)\n",
    "print(results_1DCNN)\n",
    "print(results_GlobMaxPool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history__LSTM_dict = history_LSTM.history\n",
    "history__1DCNN_dict = history_1DCNN.history\n",
    "history__GlobMaxPool_dict = history_GlobMaxPool.history\n",
    "print(history__LSTM_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들\n",
    "print(history__1DCNN_dict.keys())\n",
    "print(history__GlobMaxPool_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc_LSTM = history__LSTM_dict['accuracy']\n",
    "val_acc_LSTM = history__LSTM_dict['val_accuracy']\n",
    "loss_LSTM = history__LSTM_dict['loss']\n",
    "val_loss_LSTM = history__LSTM_dict['val_loss']\n",
    "\n",
    "acc_1DCNN = history__1DCNN_dict['accuracy']\n",
    "val_acc_1DCNN = history__1DCNN_dict['val_accuracy']\n",
    "loss_1DCNN = history__1DCNN_dict['loss']\n",
    "val_loss_1DCNN = history__1DCNN_dict['val_loss']\n",
    "\n",
    "acc_GlobMaxPool = history__GlobMaxPool_dict['accuracy']\n",
    "val_acc_GlobMaxPool= history__GlobMaxPool_dict['val_accuracy']\n",
    "loss_GlobMaxPool = history__GlobMaxPool_dict['loss']\n",
    "val_loss_GlobMaxPool = history__GlobMaxPool_dict['val_loss']\n",
    "\n",
    "epochs_LSTM = range(1, len(acc_LSTM) + 1)\n",
    "epochs_1DCNN = range(1, len(acc_1DCNN) + 1)\n",
    "epochs_GlobMaxPool = range(1, len(acc_GlobMaxPool) + 1)\n",
    "\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.subplot(2,2,1)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.plot(epochs_LSTM, loss_LSTM, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs_LSTM, val_loss_LSTM, 'b', label='Validation loss')\n",
    "plt.title('LSTM Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.plot(epochs_1DCNN, loss_1DCNN, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs_1DCNN, val_loss_1DCNN, 'b', label='Validation loss')\n",
    "plt.title('1DCNN Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.plot(epochs_GlobMaxPool, loss_GlobMaxPool, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs_GlobMaxPool, val_loss_GlobMaxPool, 'b', label='Validation loss')\n",
    "plt.title('GlobMaxPool Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.plot(epochs_LSTM, acc_LSTM, 'bo', label='Training acc')\n",
    "plt.plot(epochs_LSTM, val_acc_LSTM, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.plot(epochs_1DCNN, acc_1DCNN, 'bo', label='Training acc')\n",
    "plt.plot(epochs_1DCNN, val_acc_1DCNN, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.plot(epochs_GlobMaxPool, acc_GlobMaxPool, 'bo', label='Training acc')\n",
    "plt.plot(epochs_GlobMaxPool, val_acc_GlobMaxPool, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 학습된 Embedding 레이어 분석\n",
    "\n",
    "- 1. 가장 많이 쓰인 단어를 Sort 하고 그 중 많이 쓰인 Top100의 불용어를 추가하였다.\n",
    "\n",
    "- 2. Train set = 100000, Test set = 46000 개로 설정하였다.\n",
    "\n",
    "- 3. Maxlen = 47로, Model의 Feature를 47로 설정하였다.\n",
    "\n",
    "- 4. 3개의 모델 모두 Epoch가 2~3 이상이면 오버피팅했다.\n",
    "\n",
    "- 5. Data는 많지만 Model이 Shallow 해서 성능이 잘 안올라 가는 것으로 분석된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.  한국어 Word2Vec 임베딩 활용하여 성능개선\n",
    " - 한국어 Word2Vec은 다음 경로에서 구할 수 있습니다.\n",
    "https://github.com/Kyubyong/wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/ko/ko.bin'\n",
    "word2vec = gensim.models.Word2Vec.load(word2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도',\n",
    "             '를','으로','자','에','와','한','하다','다','고','하','을','.','..',\n",
    "             ',','었','만','는데','로','음','것','아','네요','어','같','했','에서','기','네','거'\n",
    "             ,'수','되','면','게','지','있','나','점','인','주','내','~','던','어요','할','겠','1'\n",
    "             ,'해','습니다','...','더','라','그','볼']\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 30185\n",
    "word_vector_dim = 200\n",
    "\n",
    "def load_data(train_data, test_data, num_words=vocab_size):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(vocab_size-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "\n",
    "vocab_size = 30185\n",
    "word_vector_dim = 200\n",
    "\n",
    "# 모델 구성\n",
    "model_1DCNN = keras.Sequential()\n",
    "model_1DCNN.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model_1DCNN.add(keras.layers.Conv1D(64, 7, activation='relu'))\n",
    "model_1DCNN.add(keras.layers.MaxPooling1D(5))\n",
    "model_1DCNN.add(keras.layers.Conv1D(64, 7, activation='relu'))\n",
    "model_1DCNN.add(keras.layers.GlobalMaxPooling1D())\n",
    "model_1DCNN.add(keras.layers.Dense(32, activation='relu'))\n",
    "model_1DCNN.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_1DCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습의 진행\n",
    "from keras import optimizers\n",
    "\n",
    "Adam = optimizers.Adam(lr=0.0003)\n",
    "\n",
    "model_1DCNN.compile(optimizer=Adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=15  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model_1DCNN.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구성\n",
    "model_LSTM = keras.Sequential()\n",
    "# [[YOUR CODE]]\n",
    "model_LSTM.add(keras.layers.Embedding(vocab_size, word_vector_dim,\n",
    "                                      embeddings_initializer=Constant(embedding_matrix),\n",
    "                                      input_length=maxlen, \n",
    "                                      trainable=True))\n",
    "model_LSTM.add(keras.layers.LSTM(512,return_sequences = True ))\n",
    "model_LSTM.add(keras.layers.LSTM(256,return_sequences = True)) \n",
    "model_LSTM.add(keras.layers.LSTM(128,return_sequences = True))#가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습\n",
    "model_LSTM.add(keras.layers.LSTM(128,return_sequences = True))\n",
    "model_LSTM.add(keras.layers.LSTM(64,return_sequences = True))\n",
    "model_LSTM.add(keras.layers.LSTM(64,return_sequences = False))\n",
    "model_LSTM.add(keras.layers.Dense(16, activation='relu'))\n",
    "model_LSTM.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam = optimizers.Adam(lr=0.0002)\n",
    "model_LSTM.compile(optimizer=Adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_LSTM = model_LSTM.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GlobMaxPool = keras.Sequential()\n",
    "model_GlobMaxPool.add(keras.layers.Embedding(vocab_size, word_vector_dim, \n",
    "                                             input_length=maxlen, \n",
    "                                             trainable=True))\n",
    "model_GlobMaxPool.add(keras.layers.GlobalMaxPooling1D())\n",
    "model_GlobMaxPool.add(keras.layers.Dense(8, activation='relu'))\n",
    "model_GlobMaxPool.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model_GlobMaxPool.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam = optimizers.Adam(lr=0.0002)\n",
    "model_GlobMaxPool.compile(optimizer=Adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_GlobMaxPool = model_GlobMaxPool.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc_LSTM = history__LSTM_dict['accuracy']\n",
    "val_acc_LSTM = history__LSTM_dict['val_accuracy']\n",
    "loss_LSTM = history__LSTM_dict['loss']\n",
    "val_loss_LSTM = history__LSTM_dict['val_loss']\n",
    "\n",
    "acc_1DCNN = history__1DCNN_dict['accuracy']\n",
    "val_acc_1DCNN = history__1DCNN_dict['val_accuracy']\n",
    "loss_1DCNN = history__1DCNN_dict['loss']\n",
    "val_loss_1DCNN = history__1DCNN_dict['val_loss']\n",
    "\n",
    "acc_GlobMaxPool = history__GlobMaxPool_dict['accuracy']\n",
    "val_acc_GlobMaxPool= history__GlobMaxPool_dict['val_accuracy']\n",
    "loss_GlobMaxPool = history__GlobMaxPool_dict['loss']\n",
    "val_loss_GlobMaxPool = history__GlobMaxPool_dict['val_loss']\n",
    "\n",
    "epochs_LSTM = range(1, len(acc_LSTM) + 1)\n",
    "epochs_1DCNN = range(1, len(acc_1DCNN) + 1)\n",
    "epochs_GlobMaxPool = range(1, len(acc_GlobMaxPool) + 1)\n",
    "\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.subplot(2,2,1)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.plot(epochs_LSTM, loss_LSTM, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs_LSTM, val_loss_LSTM, 'b', label='Validation loss')\n",
    "plt.title('LSTM Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.plot(epochs_1DCNN, loss_1DCNN, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs_1DCNN, val_loss_1DCNN, 'b', label='Validation loss')\n",
    "plt.title('1DCNN Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.plot(epochs_GlobMaxPool, loss_GlobMaxPool, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs_GlobMaxPool, val_loss_GlobMaxPool, 'b', label='Validation loss')\n",
    "plt.title('GlobMaxPool Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.plot(epochs_LSTM, acc_LSTM, 'bo', label='Training acc')\n",
    "plt.plot(epochs_LSTM, val_acc_LSTM, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.plot(epochs_1DCNN, acc_1DCNN, 'bo', label='Training acc')\n",
    "plt.plot(epochs_1DCNN, val_acc_1DCNN, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.plot(epochs_GlobMaxPool, acc_GlobMaxPool, 'bo', label='Training acc')\n",
    "plt.plot(epochs_GlobMaxPool, val_acc_GlobMaxPool, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
