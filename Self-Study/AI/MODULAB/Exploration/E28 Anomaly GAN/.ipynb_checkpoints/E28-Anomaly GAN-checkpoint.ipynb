{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 32, 32, 1)\n",
      "(10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Fashion MNIST padding to 32 X 32\n",
    "train_data_32 = np.zeros((train_data.shape[0], 32, 32)).astype('float32')\n",
    "test_data_32 = np.zeros((test_data.shape[0], 32, 32)).astype('float32')     \n",
    "train_data_32[:, 2:30, 2:30] = train_data\n",
    "test_data_32[:, 2:30, 2:30] = test_data\n",
    "\n",
    "# 1channel data reshape\n",
    "train_data = train_data_32.reshape(train_data_32.shape[0], 32, 32, 1).astype('float32')\n",
    "test_data = test_data_32.reshape(test_data_32.shape[0], 32, 32, 1).astype('float32')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i].reshape(32, 32), cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 8:   # Bag:8\n",
    "            new_t_labels.append([0])  # Bag을 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])  # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 32, 32, 1) (54000, 1)\n",
      "(6000, 32, 32, 1) (6000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 32, 32, 1)\n",
      "(16000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 1)\n",
      "(16000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=1)  # Generator가 32X32X1 짜리 이미지를 생성해야 합니다. \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/ganomaly_skip_no_norm/ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 100, \t Total Gen Loss : 17.667964935302734, \t Total Dis Loss : 0.6817658543586731\n",
      "Steps : 200, \t Total Gen Loss : 17.739652633666992, \t Total Dis Loss : 0.6490812301635742\n",
      "Steps : 300, \t Total Gen Loss : 20.997024536132812, \t Total Dis Loss : 0.45403537154197693\n",
      "Steps : 400, \t Total Gen Loss : 19.356504440307617, \t Total Dis Loss : 0.24051550030708313\n",
      "Steps : 500, \t Total Gen Loss : 21.696420669555664, \t Total Dis Loss : 0.263476699590683\n",
      "Steps : 600, \t Total Gen Loss : 20.084087371826172, \t Total Dis Loss : 0.28785812854766846\n",
      "Steps : 700, \t Total Gen Loss : 21.48101043701172, \t Total Dis Loss : 0.13833242654800415\n",
      "Steps : 800, \t Total Gen Loss : 23.500642776489258, \t Total Dis Loss : 0.08148667216300964\n",
      "Steps : 900, \t Total Gen Loss : 21.852067947387695, \t Total Dis Loss : 0.06323301792144775\n",
      "Steps : 1000, \t Total Gen Loss : 23.013259887695312, \t Total Dis Loss : 0.1165475845336914\n",
      "Steps : 1100, \t Total Gen Loss : 22.811859130859375, \t Total Dis Loss : 0.2874448299407959\n",
      "Steps : 1200, \t Total Gen Loss : 22.576757431030273, \t Total Dis Loss : 0.10600429773330688\n",
      "Steps : 1300, \t Total Gen Loss : 24.2196102142334, \t Total Dis Loss : 0.030056960880756378\n",
      "Steps : 1400, \t Total Gen Loss : 24.293851852416992, \t Total Dis Loss : 0.054082758724689484\n",
      "Steps : 1500, \t Total Gen Loss : 22.729778289794922, \t Total Dis Loss : 0.23548880219459534\n",
      "Steps : 1600, \t Total Gen Loss : 24.406049728393555, \t Total Dis Loss : 0.06273646652698517\n",
      "Steps : 1700, \t Total Gen Loss : 24.76422691345215, \t Total Dis Loss : 0.029339537024497986\n",
      "Steps : 1800, \t Total Gen Loss : 24.697601318359375, \t Total Dis Loss : 0.07328625023365021\n",
      "Steps : 1900, \t Total Gen Loss : 24.619352340698242, \t Total Dis Loss : 0.017282230779528618\n",
      "Steps : 2000, \t Total Gen Loss : 25.917076110839844, \t Total Dis Loss : 0.011231516487896442\n",
      "Steps : 2100, \t Total Gen Loss : 23.100805282592773, \t Total Dis Loss : 0.22563186287879944\n",
      "Steps : 2200, \t Total Gen Loss : 23.49897575378418, \t Total Dis Loss : 0.052527666091918945\n",
      "Steps : 2300, \t Total Gen Loss : 22.9578857421875, \t Total Dis Loss : 0.05029308795928955\n",
      "Steps : 2400, \t Total Gen Loss : 26.721982955932617, \t Total Dis Loss : 0.21916921436786652\n",
      "Steps : 2500, \t Total Gen Loss : 27.066356658935547, \t Total Dis Loss : 0.010651938617229462\n",
      "Steps : 2600, \t Total Gen Loss : 26.589345932006836, \t Total Dis Loss : 0.015173868276178837\n",
      "Steps : 2700, \t Total Gen Loss : 25.667909622192383, \t Total Dis Loss : 0.01620408520102501\n",
      "Steps : 2800, \t Total Gen Loss : 26.878864288330078, \t Total Dis Loss : 0.019276883453130722\n",
      "Steps : 2900, \t Total Gen Loss : 25.79769515991211, \t Total Dis Loss : 0.013667229562997818\n",
      "Steps : 3000, \t Total Gen Loss : 25.329191207885742, \t Total Dis Loss : 0.010050000622868538\n",
      "Steps : 3100, \t Total Gen Loss : 21.499128341674805, \t Total Dis Loss : 0.04327600076794624\n",
      "Steps : 3200, \t Total Gen Loss : 24.641342163085938, \t Total Dis Loss : 0.0743425041437149\n",
      "Steps : 3300, \t Total Gen Loss : 24.87459373474121, \t Total Dis Loss : 0.009860003367066383\n",
      "Steps : 3400, \t Total Gen Loss : 22.57872200012207, \t Total Dis Loss : 0.2348969578742981\n",
      "Steps : 3500, \t Total Gen Loss : 23.941362380981445, \t Total Dis Loss : 0.00899992324411869\n",
      "Steps : 3600, \t Total Gen Loss : 26.375152587890625, \t Total Dis Loss : 0.0077857221476733685\n",
      "Steps : 3700, \t Total Gen Loss : 23.44552993774414, \t Total Dis Loss : 0.017832133919000626\n",
      "Steps : 3800, \t Total Gen Loss : 22.182228088378906, \t Total Dis Loss : 0.032466769218444824\n",
      "Steps : 3900, \t Total Gen Loss : 26.055927276611328, \t Total Dis Loss : 0.012546185404062271\n",
      "Steps : 4000, \t Total Gen Loss : 24.97869110107422, \t Total Dis Loss : 0.022915158420801163\n",
      "Steps : 4100, \t Total Gen Loss : 23.566532135009766, \t Total Dis Loss : 0.037288863211870193\n",
      "Steps : 4200, \t Total Gen Loss : 24.823579788208008, \t Total Dis Loss : 0.011064963415265083\n",
      "Steps : 4300, \t Total Gen Loss : 27.346376419067383, \t Total Dis Loss : 0.01042251754552126\n",
      "Steps : 4400, \t Total Gen Loss : 27.298290252685547, \t Total Dis Loss : 0.0035881237126886845\n",
      "Steps : 4500, \t Total Gen Loss : 28.52968406677246, \t Total Dis Loss : 0.005779827479273081\n",
      "Steps : 4600, \t Total Gen Loss : 25.602537155151367, \t Total Dis Loss : 0.007110121659934521\n",
      "Steps : 4700, \t Total Gen Loss : 22.023900985717773, \t Total Dis Loss : 1.4073207378387451\n",
      "Steps : 4800, \t Total Gen Loss : 25.809019088745117, \t Total Dis Loss : 0.00829241331666708\n",
      "Steps : 4900, \t Total Gen Loss : 23.831066131591797, \t Total Dis Loss : 0.08044331520795822\n",
      "Steps : 5000, \t Total Gen Loss : 24.40545654296875, \t Total Dis Loss : 0.006348798051476479\n",
      "Steps : 5100, \t Total Gen Loss : 26.8737850189209, \t Total Dis Loss : 0.0021867852192372084\n",
      "Steps : 5200, \t Total Gen Loss : 27.22066879272461, \t Total Dis Loss : 0.003673698753118515\n",
      "Steps : 5300, \t Total Gen Loss : 23.897686004638672, \t Total Dis Loss : 0.03169439360499382\n",
      "Steps : 5400, \t Total Gen Loss : 28.289979934692383, \t Total Dis Loss : 0.011416737921535969\n",
      "Steps : 5500, \t Total Gen Loss : 25.49465560913086, \t Total Dis Loss : 0.0029930584132671356\n",
      "Steps : 5600, \t Total Gen Loss : 26.66436767578125, \t Total Dis Loss : 0.02944866567850113\n",
      "Steps : 5700, \t Total Gen Loss : 26.695716857910156, \t Total Dis Loss : 0.004683270119130611\n",
      "Steps : 5800, \t Total Gen Loss : 25.195755004882812, \t Total Dis Loss : 0.004943787585943937\n",
      "Steps : 5900, \t Total Gen Loss : 29.753643035888672, \t Total Dis Loss : 0.0044427854008972645\n",
      "Steps : 6000, \t Total Gen Loss : 25.950578689575195, \t Total Dis Loss : 0.0032023494131863117\n",
      "Steps : 6100, \t Total Gen Loss : 24.224109649658203, \t Total Dis Loss : 0.016290634870529175\n",
      "Steps : 6200, \t Total Gen Loss : 24.90106964111328, \t Total Dis Loss : 0.005818110890686512\n",
      "Steps : 6300, \t Total Gen Loss : 26.16884994506836, \t Total Dis Loss : 0.00867113471031189\n",
      "Steps : 6400, \t Total Gen Loss : 28.80392074584961, \t Total Dis Loss : 0.0016683111898601055\n",
      "Steps : 6500, \t Total Gen Loss : 25.944917678833008, \t Total Dis Loss : 0.004460850264877081\n",
      "Steps : 6600, \t Total Gen Loss : 29.147693634033203, \t Total Dis Loss : 0.002321496605873108\n",
      "Steps : 6700, \t Total Gen Loss : 27.30680274963379, \t Total Dis Loss : 0.003360852599143982\n",
      "Time for epoch 1 is 350.6778829097748 sec\n",
      "Steps : 6800, \t Total Gen Loss : 28.39320182800293, \t Total Dis Loss : 0.0033583641052246094\n",
      "Steps : 6900, \t Total Gen Loss : 28.101924896240234, \t Total Dis Loss : 0.002228384604677558\n",
      "Steps : 7000, \t Total Gen Loss : 27.74400520324707, \t Total Dis Loss : 0.0031746425665915012\n",
      "Steps : 7100, \t Total Gen Loss : 28.63128089904785, \t Total Dis Loss : 0.009885004721581936\n",
      "Steps : 7200, \t Total Gen Loss : 26.888042449951172, \t Total Dis Loss : 0.0015172016574069858\n",
      "Steps : 7300, \t Total Gen Loss : 26.484006881713867, \t Total Dis Loss : 0.06962403655052185\n",
      "Steps : 7400, \t Total Gen Loss : 25.46364974975586, \t Total Dis Loss : 0.0016912140417844057\n",
      "Steps : 7500, \t Total Gen Loss : 26.991544723510742, \t Total Dis Loss : 0.006996884010732174\n",
      "Steps : 7600, \t Total Gen Loss : 26.195070266723633, \t Total Dis Loss : 0.006063979584723711\n",
      "Steps : 7700, \t Total Gen Loss : 29.877511978149414, \t Total Dis Loss : 0.0010497277835384011\n",
      "Steps : 7800, \t Total Gen Loss : 27.588722229003906, \t Total Dis Loss : 0.0016622607363387942\n",
      "Steps : 7900, \t Total Gen Loss : 26.341617584228516, \t Total Dis Loss : 0.007069881074130535\n",
      "Steps : 8000, \t Total Gen Loss : 27.04393196105957, \t Total Dis Loss : 0.008588052354753017\n",
      "Steps : 8100, \t Total Gen Loss : 31.913625717163086, \t Total Dis Loss : 0.023216083645820618\n",
      "Steps : 8200, \t Total Gen Loss : 28.90707778930664, \t Total Dis Loss : 0.0035085566341876984\n",
      "Steps : 8300, \t Total Gen Loss : 30.036354064941406, \t Total Dis Loss : 0.003939099609851837\n",
      "Steps : 8400, \t Total Gen Loss : 28.387365341186523, \t Total Dis Loss : 0.0021919822320342064\n",
      "Steps : 8500, \t Total Gen Loss : 27.571197509765625, \t Total Dis Loss : 0.015706688165664673\n",
      "Steps : 8600, \t Total Gen Loss : 28.55614471435547, \t Total Dis Loss : 0.006570474710315466\n",
      "Steps : 8700, \t Total Gen Loss : 27.47549057006836, \t Total Dis Loss : 0.005378423724323511\n",
      "Steps : 8800, \t Total Gen Loss : 28.929372787475586, \t Total Dis Loss : 0.0013607924338430166\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 5\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이상감지용 데이터셋 구축 (개구리 데이터를 학습데이터셋에서 제외하여 테스트 데이터셋에 포함)\n",
    "- Skip-GANomaly 모델의 구현\n",
    "- 모델의 학습과 검증\n",
    "- 검증 결과의 시각화 (정상-이상 데이터의 anomaly score 분포 시각화, 적절한 threshold에 따른 이삼감지율 계-산, 감지 성공/실패사례 시각화 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
