{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq 인코더-디코더 구조\n",
    "\n",
    "![img](E15im1.jpg)\n",
    "\n",
    "Encoder = 데이터에서 Feature Extractor\n",
    "Decoder = 데이터 재생성\n",
    "\n",
    "![img](E15im2.png)\n",
    "\n",
    "Seq2seq의 feature vector ? ==> 인코더의 입력 문장을 해석해서 만든 Hidden state vector\n",
    "\n",
    "- A언어의 문장 X를 z라는 hidden state로 해석한 후 z를 다시 B 언어의 문장 Y로 재생성하는 것입니다. 그러므로 인코더에서 필요한 것은 인코더의 마지막 time step의 hidden state입니다. 그리고 이를 두번째 RNN인 디코더에 전달합니다.\n",
    "\n",
    "- 디코더는 인코더의 마지막 time step의 hidden state를 전달 받아 자신의 초기 hidden state로 하고, 출력 문장을 생성해내기 시작합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Language Model\n",
    "### 어떤 말을 만들고 싶은지 제어하기 위한 언어 모델 = 번역(seq2seq)\n",
    "- 문장 X를 해석해서 c로 만드는 인코더를 또다른 RNN으로 만드는 것입니다. 그렇게 만든 c를 다시 문장생성기인 디코더 RNN에 입력으로 넣어주는 모델을 만들어 낸 것이 바로 오늘 다루게 될 seq2seq인 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Forcing(교사 강요)\n",
    "\n",
    "\n",
    "![img](E15im3.png)\n",
    "\n",
    "- seq2seq는 훈련 과정과 테스트 과정에서의 동작 방식이 다르다는 특징이 있습니다. 이전 스텝의 그림을 보면 디코더 RNN은 이전 time step의 출력을 현재 time step의 입력으로 사용한다는 특징을 가지고 있습니다. 그런데 이는 테스트 과정에서의 이야기이고, 훈련 과정은 조금 다른 방식을 사용합니다. 그 이유는 훈련 과정에서 이전 time step이 잘못된 예측을 한다면 이를 입력으로 한 현재 time step의 예측도 잘못될 수 있기 때문입니다. 이런 상황이 반복되면 훈련 시간이 굉장히 늘어나게 됩니다.\n",
    "\n",
    "- 훈련 과정에서는 실제 정답 시퀀스를 알고 있는 상황이므로 이전 time step의 예측값을 현재 time step의 입력으로 사용하는 것이 아니라 이전 time step의 실제값으로 사용할 수 있습니다. 이 작업을 교사 강요(teacher forcing)이라고 합니다. 이 기법은 seq2seq 뿐 아니라 sequence 데이터의 생성모델에서 일반적으로 사용되는 기법이기도 합니다. 물론, 이는 모델이 훈련 데이터 외의 결과를 생성해내는 능력을 기르는데에 조금 방해가 될 수 있다는 단점도 존재합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 번역기 만들기\n",
    "\n",
    "## 1. 데이터 전처리\n",
    "\n",
    "```\n",
    "$ mkdir -p ~/aiffel/translator_seq2seq/data\n",
    "$ mkdir -p ~/aiffel/translator_seq2seq/models\n",
    "$ wget https://www.manythings.org/anki/fra-eng.zip\n",
    "$ mv fra-eng.zip  ~/aiffel/translator_seq2seq/data\n",
    "$ cd ~/aiffel/translator_seq2seq/data && unzip fra-eng.zip\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 178009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165758</th>\n",
       "      <td>She got him to do everything she wanted him to...</td>\n",
       "      <td>Elle a obtenu qu'il fasse tout ce qu'elle voul...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109075</th>\n",
       "      <td>Did you do yesterday's homework?</td>\n",
       "      <td>As-tu fait les devoirs d'hier ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133797</th>\n",
       "      <td>Why did you become a police officer?</td>\n",
       "      <td>Pourquoi êtes-vous devenu agent de police ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24280</th>\n",
       "      <td>I hate these words.</td>\n",
       "      <td>Je déteste ces mots.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103367</th>\n",
       "      <td>He likes geography and history.</td>\n",
       "      <td>Il aime la géographie et l'histoire.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "165758  She got him to do everything she wanted him to...   \n",
       "109075                   Did you do yesterday's homework?   \n",
       "133797               Why did you become a police officer?   \n",
       "24280                                 I hate these words.   \n",
       "103367                    He likes geography and history.   \n",
       "\n",
       "                                                      fra  \\\n",
       "165758  Elle a obtenu qu'il fasse tout ce qu'elle voul...   \n",
       "109075                    As-tu fait les devoirs d'hier ?   \n",
       "133797        Pourquoi êtes-vous devenu agent de police ?   \n",
       "24280                                Je déteste ces mots.   \n",
       "103367               Il aime la géographie et l'histoire.   \n",
       "\n",
       "                                                       cc  \n",
       "165758  CC-BY 2.0 (France) Attribution: tatoeba.org #8...  \n",
       "109075  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "133797  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "24280   CC-BY 2.0 (France) Attribution: tatoeba.org #8...  \n",
       "103367  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>We all quit.</td>\n",
       "      <td>Nous arrêtons tous.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4904</th>\n",
       "      <td>Are they dead?</td>\n",
       "      <td>Sont-elles mortes ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11306</th>\n",
       "      <td>I remember that.</td>\n",
       "      <td>Je me souviens de cela.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18493</th>\n",
       "      <td>Get on your knees.</td>\n",
       "      <td>Mets-toi à genoux.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12818</th>\n",
       "      <td>Tom is likeable.</td>\n",
       "      <td>Tom est sympathique.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      eng                      fra\n",
       "2975         We all quit.      Nous arrêtons tous.\n",
       "4904       Are they dead?      Sont-elles mortes ?\n",
       "11306    I remember that.  Je me souviens de cela.\n",
       "18493  Get on your knees.       Mets-toi à genoux.\n",
       "12818    Tom is likeable.     Tom est sympathique."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34893</th>\n",
       "      <td>He was fully clothed.</td>\n",
       "      <td>\\t Il était entièrement vêtu. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>Have a donut.</td>\n",
       "      <td>\\t Prends un beignet. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21524</th>\n",
       "      <td>Tom broke his arm.</td>\n",
       "      <td>\\t Tom lui a cassé le bras. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16708</th>\n",
       "      <td>They've all left.</td>\n",
       "      <td>\\t Elles sont toutes parties. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20754</th>\n",
       "      <td>She can jump high.</td>\n",
       "      <td>\\t Elle peut sauter haut. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         eng                               fra\n",
       "34893  He was fully clothed.  \\t Il était entièrement vêtu. \\n\n",
       "3460           Have a donut.          \\t Prends un beignet. \\n\n",
       "21524     Tom broke his arm.    \\t Tom lui a cassé le bras. \\n\n",
       "16708      They've all left.  \\t Elles sont toutes parties. \\n\n",
       "20754     She can jump high.      \\t Elle peut sauter haut. \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스틑 -> 정수 로 변환하는 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 3, 8], [10, 5, 8], [10, 5, 8]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 1, 19, 4, 1, 33, 1, 12],\n",
       " [11, 1, 3, 4, 13, 7, 5, 1, 33, 1, 12],\n",
       " [11, 1, 3, 4, 13, 7, 5, 14, 1, 12]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0번 토큰을 고려하여 size +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 51\n",
      "프랑스어 단어장의 크기 : 73\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 51\n",
      "프랑스어 단어장의 크기 : 73\n",
      "영어 시퀀스의 최대 길이 23\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어  : 인코더의 입력\n",
    "### 프랑스어 : 1. 디코더의 출력과 비교해야할 정답 데이터         2. Teacher Forcing 을 위해 디코더의 입력으로 사용할 데이터\n",
    "\n",
    "\n",
    "- 디코더의 입력으로 사용할 시퀀스는 < eos >토큰이 필요가 없고, 디코더의 출력과 비교할 시퀀스는 < sos >가 필요가 없기 때문입니다. 가령, 영어로 'I am a person'이라는 문장을 프랑스어 'Je suis une personne'로 번역하는 번역기를 만든다고 해봅시다. 훈련 과정에서 디코더는 '< sos > Je suis une personne'를 입력받아서 'Je suis une personne < eos >'를 예측하도록 훈련되므로, 이런 방식으로 생성된 두가지 버전의 시퀀스를 준비해야 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 1, 19, 4, 1, 33, 1], [11, 1, 3, 4, 13, 7, 5, 1, 33, 1], [11, 1, 3, 4, 13, 7, 5, 14, 1]]\n",
      "[[1, 19, 4, 1, 33, 1, 12], [1, 3, 4, 13, 7, 5, 1, 33, 1, 12], [1, 3, 4, 13, 7, 5, 14, 1, 12]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 23)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  3  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 23, 51)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5만건 중, 3만건만 검증 데이터로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (50000, 23, 51)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 설계하기\n",
    " 1. LSTM 셀 설계\n",
    " 2. 문장 입력\n",
    " 3. 마지막 Time step의 Hideen state 와 cell state를 저장\n",
    " \n",
    "\n",
    "- encoder의 hidden state는 디코더의 첫번째 hidden state\n",
    " - LSTM이라 cell state 까지 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "# 디코더로 전달할 hidden state, cell state를 리턴. encoder_outputs은 여기서는 불필요.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code 분석\n",
    "1. 우선 LSTM의 입력 텐서를 정의해줍니다. 입력 문장을 저장하게 될 변수 텐서입니다.\n",
    "\n",
    "2. 256의 hidden_size를 가지는 LSTM 셀을 만들어줍니다. LSTM의 수용력(capacity)를 의미합니다. return_state = True를 해서 hidden state와 cell state를 리턴받을 수 있도록 합니다.\n",
    "\n",
    "3. 입력 텐서를 입력으로 마지막 time step의 hidden state와 cell state를 결과로 받습니다.\n",
    "\n",
    "4. 마지막 time step의 hidden state와 cell state를 encoder_states라는 하나의 변수에 저장해뒀습니다. 이를 디코더에 전달하면 되겠네요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoer 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 세번째 줄을 보면 디코더의 인자로 initial_state : LSTM 셀의 초기 상태를 정의해주는 인자. (여기서는 Encoder의 마지막 time step의 hidden , cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 51)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 315392      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 672,073\n",
      "Trainable params: 672,073\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.9401 - val_loss: 0.8105\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.5720 - val_loss: 0.6464\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.4670 - val_loss: 0.5683\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.4101 - val_loss: 0.5086\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.3722 - val_loss: 0.4792\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.3448 - val_loss: 0.4518\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.3236 - val_loss: 0.4390\n",
      "Epoch 8/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.3071 - val_loss: 0.4238\n",
      "Epoch 9/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2934 - val_loss: 0.4055\n",
      "Epoch 10/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2822 - val_loss: 0.4020\n",
      "Epoch 11/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2724 - val_loss: 0.4060\n",
      "Epoch 12/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2638 - val_loss: 0.3919\n",
      "Epoch 13/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2563 - val_loss: 0.3889\n",
      "Epoch 14/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2495 - val_loss: 0.3800\n",
      "Epoch 15/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2433 - val_loss: 0.3725\n",
      "Epoch 16/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2377 - val_loss: 0.3735\n",
      "Epoch 17/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2324 - val_loss: 0.3741\n",
      "Epoch 18/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2276 - val_loss: 0.3681\n",
      "Epoch 19/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2231 - val_loss: 0.3671\n",
      "Epoch 20/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2188 - val_loss: 0.3686\n",
      "Epoch 21/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2149 - val_loss: 0.3677\n",
      "Epoch 22/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2111 - val_loss: 0.3719\n",
      "Epoch 23/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2074 - val_loss: 0.3659\n",
      "Epoch 24/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2040 - val_loss: 0.3694\n",
      "Epoch 25/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2008 - val_loss: 0.3685\n",
      "Epoch 26/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1978 - val_loss: 0.3683\n",
      "Epoch 27/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1948 - val_loss: 0.3662\n",
      "Epoch 28/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1918 - val_loss: 0.3688\n",
      "Epoch 29/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1891 - val_loss: 0.3718\n",
      "Epoch 30/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1864 - val_loss: 0.3709\n",
      "Epoch 31/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1838 - val_loss: 0.3699\n",
      "Epoch 32/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1812 - val_loss: 0.3698\n",
      "Epoch 33/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1790 - val_loss: 0.3757\n",
      "Epoch 34/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1767 - val_loss: 0.3806\n",
      "Epoch 35/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1744 - val_loss: 0.3743\n",
      "Epoch 36/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1722 - val_loss: 0.3781\n",
      "Epoch 37/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1700 - val_loss: 0.3840\n",
      "Epoch 38/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1680 - val_loss: 0.3812\n",
      "Epoch 39/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1659 - val_loss: 0.3928\n",
      "Epoch 40/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1642 - val_loss: 0.3871\n",
      "Epoch 41/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1624 - val_loss: 0.3880\n",
      "Epoch 42/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1606 - val_loss: 0.3917\n",
      "Epoch 43/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1586 - val_loss: 0.3913\n",
      "Epoch 44/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1569 - val_loss: 0.3947\n",
      "Epoch 45/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1552 - val_loss: 0.3972\n",
      "Epoch 46/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1537 - val_loss: 0.3995\n",
      "Epoch 47/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1521 - val_loss: 0.4009\n",
      "Epoch 48/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1505 - val_loss: 0.4012\n",
      "Epoch 49/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1490 - val_loss: 0.4075\n",
      "Epoch 50/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1476 - val_loss: 0.4101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0ffcba6d90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 테스트 하기\n",
    "1. 인코더에 입력 문장을 넣어 마지막 time step의 hidden, cell state를 얻는다.\n",
    "2. 토큰인 '\\t'를 디코더에 입력한다.\n",
    "3. 이전 time step의 출력층의 예측 결과를 현재 time step의 입력으로 한다.\n",
    "4. 3을 반복하다가 토큰인 '\\n'가 예측되면 이를 중단한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 51)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 315392    \n",
      "=================================================================\n",
      "Total params: 315,392\n",
      "Trainable params: 315,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder 출력층 재설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 356,681\n",
      "Trainable params: 356,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary 생성 ( 단어 -> 정수 , 정수-> 단어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decode_sequence()의 입력으로 들어가는 것은 번역하고자 하는 문장의 정수 시퀀스입니다. decode_sequence() 내부에는 인코더를 구현한 encoder_model이 있어서 이 모델에 번역하고자 하는 문장의 정수 시퀀스인 'input_seq'를 입력하면, encoder_model은 마지막 시점의 hidden state를 리턴합니다.\n",
    "- hidden state는 디코더의 첫번째 시점의 hidden state가 되고, 디코더는 이제 번역 문장을 완성하기 위한 예측 과정을 진행합니다. 디코더의 예측 과정에서는 이전 시점에서 예측한 단어를 디코더의 현재 시점의 입력으로 넣어주는 작업을 진행합니다. 그리고 이 작업은 종료를 의미하는 종료 토큰을 만나거나, 주어진 최대 길이를 넘을 때까지 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장:  Cours ! \n",
      "번역기가 번역한 문장:  cours ! \n",
      "-----------------------------------\n",
      "입력 문장: I left.\n",
      "정답 문장:  Je suis partie. \n",
      "번역기가 번역한 문장:  je suis tombée. \n",
      "-----------------------------------\n",
      "입력 문장: Call us.\n",
      "정답 문장:  Appelez-nous ! \n",
      "번역기가 번역한 문장:  appelez-vous ! \n",
      "-----------------------------------\n",
      "입력 문장: How nice!\n",
      "정답 문장:  Comme c'est gentil ! \n",
      "번역기가 번역한 문장:  comme c'est blanc ! \n",
      "-----------------------------------\n",
      "입력 문장: Turn left.\n",
      "정답 문장:  Tourne à gauche. \n",
      "번역기가 번역한 문장:  tourne à gauche. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 데이터에서 상위 33,000개의 샘플만 사용해주세요\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 정제, 정규화, 전처리 (영어, 프랑스어)\n",
    "\n",
    "### 1-1. 구두점(Punctuation)을 단어와 분리\n",
    "\n",
    "- 분리 전 : he is a Good boy!\n",
    "\n",
    "- 분리 후 : he is a Good boy !\n",
    "\n",
    "### 2. 소문자로 변환\n",
    "\n",
    "- 변환 전 : he is a Good boy !\n",
    "\n",
    "- 변환 후 : he is a good boy !\n",
    "\n",
    "### 3. 띄어쓰기 단위로 토큰를 수행하세요.\n",
    "\n",
    "- 토큰화 전 : 'he is a good boy !'\n",
    "\n",
    "- 토큰화 후 : ['he', 'is', 'a', 'good', 'boy', '!']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 디코더의 문장에 시작 토큰과 종료 토큰을 넣어기\n",
    "\n",
    "- 입력 시퀀스 : ['', 'courez', '!']\n",
    "\n",
    "- 레이블 시퀀스 : ['courez', '!', ']\n",
    "\n",
    "## 3. 케라스의 토크나이저로 텍스트를 숫자로 바꾸기\n",
    "\n",
    " - 아래 링크의 2. 케라스 텍스트 전처리 참고\n",
    "https://wikidocs.net/31766\n",
    "\n",
    "- 영어와 프랑스어에 대한 토크나이저를 각각 생성하고, tokenizer.texts_to_sequences()를 사용하여 모든 샘플에 대해서 정수 시퀀스로 변환\n",
    "\n",
    "## 4. 임베딩 층(Embedding layer) 사용하기\n",
    " - 입력이 되는 각 단어를 임베딩 층을 사용해서 벡터화 하기. 아래 링크의 1. 케라스 임베딩층 참고\n",
    "\n",
    " - https://wikidocs.net/33793 \n",
    " \n",
    " - 코드 예시)\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Input, mbedding, Masking\n",
    "\n",
    "# 인코더에서 사용할 임베딩 층 사용 예시\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(단어장의 크기, 임베딩 벡터의 차원)(encoder_inputs)\n",
    "encoder_lstm = LSTM(hidden state의 크기, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "```\n",
    "\n",
    "#### 주의할 점은 인코더와 디코더의 임베딩 층은 서로 다른 임베딩 층을 사용해야 하지만,\n",
    "\n",
    "디코더의 훈련 과정과 테스트 과정(예측 과정)에서의 임베딩 층은 동일해야 합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 구현하기\n",
    "\n",
    "## 6. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
