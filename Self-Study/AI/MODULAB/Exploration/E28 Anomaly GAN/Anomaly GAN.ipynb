{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 32, 32, 1)\n",
      "(10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Fashion MNIST padding to 32 X 32\n",
    "train_data_32 = np.zeros((train_data.shape[0], 32, 32)).astype('float32')\n",
    "test_data_32 = np.zeros((test_data.shape[0], 32, 32)).astype('float32')     \n",
    "train_data_32[:, 2:30, 2:30] = train_data\n",
    "test_data_32[:, 2:30, 2:30] = test_data\n",
    "\n",
    "# 1channel data reshape\n",
    "train_data = train_data_32.reshape(train_data_32.shape[0], 32, 32, 1).astype('float32')\n",
    "test_data = test_data_32.reshape(test_data_32.shape[0], 32, 32, 1).astype('float32')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i].reshape(32, 32), cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 8:   # Bag:8\n",
    "            new_t_labels.append([0])  # Bag을 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])  # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 32, 32, 1) (54000, 1)\n",
      "(6000, 32, 32, 1) (6000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 32, 32, 1)\n",
      "(16000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 1)\n",
      "(16000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=1)  # Generator가 32X32X1 짜리 이미지를 생성해야 합니다. \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/ganomaly_skip_no_norm/ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 100, \t Total Gen Loss : 30.37959861755371, \t Total Dis Loss : 0.0015587876550853252\n",
      "Steps : 200, \t Total Gen Loss : 30.332420349121094, \t Total Dis Loss : 0.003452947363257408\n",
      "Steps : 300, \t Total Gen Loss : 29.90044593811035, \t Total Dis Loss : 0.0003504113119561225\n",
      "Steps : 400, \t Total Gen Loss : 30.034093856811523, \t Total Dis Loss : 0.0004987683496437967\n",
      "Steps : 500, \t Total Gen Loss : 31.128765106201172, \t Total Dis Loss : 4.926184919895604e-05\n",
      "Steps : 600, \t Total Gen Loss : 28.205957412719727, \t Total Dis Loss : 0.0006169606349430978\n",
      "Steps : 700, \t Total Gen Loss : 30.47907829284668, \t Total Dis Loss : 0.00026139229885302484\n",
      "Steps : 800, \t Total Gen Loss : 30.661102294921875, \t Total Dis Loss : 9.76066367002204e-05\n",
      "Steps : 900, \t Total Gen Loss : 31.167226791381836, \t Total Dis Loss : 0.0015922600869089365\n",
      "Steps : 1000, \t Total Gen Loss : 30.250551223754883, \t Total Dis Loss : 0.0008385974797420204\n",
      "Steps : 1100, \t Total Gen Loss : 27.18029022216797, \t Total Dis Loss : 0.013124171644449234\n",
      "Steps : 1200, \t Total Gen Loss : 30.49480438232422, \t Total Dis Loss : 0.0002557030238676816\n",
      "Steps : 1300, \t Total Gen Loss : 29.02297019958496, \t Total Dis Loss : 0.0011792001314461231\n",
      "Steps : 1400, \t Total Gen Loss : 28.947864532470703, \t Total Dis Loss : 0.000706529535818845\n",
      "Steps : 1500, \t Total Gen Loss : 29.399770736694336, \t Total Dis Loss : 0.0013092195149511099\n",
      "Steps : 1600, \t Total Gen Loss : 29.51966094970703, \t Total Dis Loss : 0.0015366256702691317\n",
      "Steps : 1700, \t Total Gen Loss : 28.696914672851562, \t Total Dis Loss : 0.000262370245764032\n",
      "Steps : 1800, \t Total Gen Loss : 29.067684173583984, \t Total Dis Loss : 0.0015849156770855188\n",
      "Steps : 1900, \t Total Gen Loss : 27.6404972076416, \t Total Dis Loss : 0.0023419447243213654\n",
      "Steps : 2000, \t Total Gen Loss : 31.106895446777344, \t Total Dis Loss : 0.0007497556507587433\n",
      "Steps : 2100, \t Total Gen Loss : 28.812355041503906, \t Total Dis Loss : 0.0008924430003389716\n",
      "Steps : 2200, \t Total Gen Loss : 27.84052276611328, \t Total Dis Loss : 0.01481290441006422\n",
      "Steps : 2300, \t Total Gen Loss : 28.732393264770508, \t Total Dis Loss : 0.0005835328483954072\n",
      "Steps : 2400, \t Total Gen Loss : 30.556957244873047, \t Total Dis Loss : 8.771283319219947e-05\n",
      "Steps : 2500, \t Total Gen Loss : 30.4729061126709, \t Total Dis Loss : 0.0003146716917399317\n",
      "Steps : 2600, \t Total Gen Loss : 31.00169563293457, \t Total Dis Loss : 0.0002991174114868045\n",
      "Steps : 2700, \t Total Gen Loss : 29.113256454467773, \t Total Dis Loss : 0.04077170789241791\n",
      "Steps : 2800, \t Total Gen Loss : 30.218881607055664, \t Total Dis Loss : 0.0019180160015821457\n",
      "Steps : 2900, \t Total Gen Loss : 29.89682388305664, \t Total Dis Loss : 0.008194906637072563\n",
      "Steps : 3000, \t Total Gen Loss : 31.966724395751953, \t Total Dis Loss : 0.011998758651316166\n",
      "Steps : 3100, \t Total Gen Loss : 32.5639762878418, \t Total Dis Loss : 0.014271016232669353\n",
      "Steps : 3200, \t Total Gen Loss : 35.057613372802734, \t Total Dis Loss : 0.0012532458640635014\n",
      "Steps : 3300, \t Total Gen Loss : 31.904552459716797, \t Total Dis Loss : 0.002535797655582428\n",
      "Steps : 3400, \t Total Gen Loss : 31.146085739135742, \t Total Dis Loss : 0.001478367019444704\n",
      "Steps : 3500, \t Total Gen Loss : 31.490909576416016, \t Total Dis Loss : 0.0010721611324697733\n",
      "Steps : 3600, \t Total Gen Loss : 31.958354949951172, \t Total Dis Loss : 0.007086847443133593\n",
      "Steps : 3700, \t Total Gen Loss : 31.0183162689209, \t Total Dis Loss : 0.002474617911502719\n",
      "Steps : 3800, \t Total Gen Loss : 36.718017578125, \t Total Dis Loss : 0.0010776437120512128\n",
      "Steps : 3900, \t Total Gen Loss : 30.516183853149414, \t Total Dis Loss : 0.007150881923735142\n",
      "Steps : 4000, \t Total Gen Loss : 31.93781852722168, \t Total Dis Loss : 0.0012457510456442833\n",
      "Steps : 4100, \t Total Gen Loss : 33.42268753051758, \t Total Dis Loss : 0.001664541196078062\n",
      "Steps : 4200, \t Total Gen Loss : 32.442352294921875, \t Total Dis Loss : 0.0010201948462054133\n",
      "Steps : 4300, \t Total Gen Loss : 32.692962646484375, \t Total Dis Loss : 0.001083962619304657\n",
      "Steps : 4400, \t Total Gen Loss : 29.63941192626953, \t Total Dis Loss : 0.04919299855828285\n",
      "Steps : 4500, \t Total Gen Loss : 32.567317962646484, \t Total Dis Loss : 0.0007069657440297306\n",
      "Steps : 4600, \t Total Gen Loss : 31.187461853027344, \t Total Dis Loss : 0.0025035913567990065\n",
      "Steps : 4700, \t Total Gen Loss : 30.321744918823242, \t Total Dis Loss : 0.0006539869937114418\n",
      "Steps : 4800, \t Total Gen Loss : 30.59073257446289, \t Total Dis Loss : 0.0003505900676827878\n",
      "Steps : 4900, \t Total Gen Loss : 30.953838348388672, \t Total Dis Loss : 0.0013961889781057835\n",
      "Steps : 5000, \t Total Gen Loss : 29.935075759887695, \t Total Dis Loss : 0.1455974280834198\n",
      "Steps : 5100, \t Total Gen Loss : 31.222604751586914, \t Total Dis Loss : 0.0034812712110579014\n",
      "Steps : 5200, \t Total Gen Loss : 33.07728576660156, \t Total Dis Loss : 0.0003975885920226574\n",
      "Steps : 5300, \t Total Gen Loss : 31.263412475585938, \t Total Dis Loss : 0.0006712758331559598\n",
      "Steps : 5400, \t Total Gen Loss : 30.178524017333984, \t Total Dis Loss : 0.0005420905654318631\n",
      "Steps : 5500, \t Total Gen Loss : 30.512680053710938, \t Total Dis Loss : 0.0010951883159577847\n",
      "Steps : 5600, \t Total Gen Loss : 31.908082962036133, \t Total Dis Loss : 0.002006552182137966\n",
      "Steps : 5700, \t Total Gen Loss : 33.76250076293945, \t Total Dis Loss : 0.0025150522124022245\n",
      "Steps : 5800, \t Total Gen Loss : 30.73331069946289, \t Total Dis Loss : 0.002600342035293579\n",
      "Steps : 5900, \t Total Gen Loss : 31.85588836669922, \t Total Dis Loss : 0.00040193667518906295\n",
      "Steps : 6000, \t Total Gen Loss : 32.12759780883789, \t Total Dis Loss : 0.002470031613484025\n",
      "Steps : 6100, \t Total Gen Loss : 30.78915786743164, \t Total Dis Loss : 0.002950538881123066\n",
      "Steps : 6200, \t Total Gen Loss : 31.253604888916016, \t Total Dis Loss : 0.017215263098478317\n",
      "Steps : 6300, \t Total Gen Loss : 28.426755905151367, \t Total Dis Loss : 0.0014761855127289891\n",
      "Steps : 6400, \t Total Gen Loss : 29.57096290588379, \t Total Dis Loss : 0.0019272988429293036\n",
      "Steps : 6500, \t Total Gen Loss : 28.347803115844727, \t Total Dis Loss : 0.0007599543314427137\n",
      "Steps : 6600, \t Total Gen Loss : 32.6335563659668, \t Total Dis Loss : 0.0004313871613703668\n",
      "Steps : 6700, \t Total Gen Loss : 33.43448257446289, \t Total Dis Loss : 0.0010597540531307459\n",
      "Time for epoch 1 is 351.70124983787537 sec\n",
      "Steps : 6800, \t Total Gen Loss : 30.720684051513672, \t Total Dis Loss : 0.015374709852039814\n",
      "Steps : 6900, \t Total Gen Loss : 32.35047149658203, \t Total Dis Loss : 0.0012545620556920767\n",
      "Steps : 7000, \t Total Gen Loss : 32.0139045715332, \t Total Dis Loss : 0.002688542939722538\n",
      "Steps : 7100, \t Total Gen Loss : 30.305618286132812, \t Total Dis Loss : 0.013578912243247032\n",
      "Steps : 7200, \t Total Gen Loss : 30.4926700592041, \t Total Dis Loss : 0.010966540314257145\n",
      "Steps : 7300, \t Total Gen Loss : 33.27408981323242, \t Total Dis Loss : 0.002719518728554249\n",
      "Steps : 7400, \t Total Gen Loss : 31.54604721069336, \t Total Dis Loss : 0.0012540637981146574\n",
      "Steps : 7500, \t Total Gen Loss : 31.612224578857422, \t Total Dis Loss : 0.0027869301848113537\n",
      "Steps : 7600, \t Total Gen Loss : 31.20252227783203, \t Total Dis Loss : 0.0015055880649015307\n",
      "Steps : 7700, \t Total Gen Loss : 32.10347366333008, \t Total Dis Loss : 0.0015410562045872211\n",
      "Steps : 7800, \t Total Gen Loss : 31.2236328125, \t Total Dis Loss : 0.0005982662551105022\n",
      "Steps : 7900, \t Total Gen Loss : 29.806833267211914, \t Total Dis Loss : 0.0017755567096173763\n",
      "Steps : 8000, \t Total Gen Loss : 30.733280181884766, \t Total Dis Loss : 0.0008159606950357556\n",
      "Steps : 8100, \t Total Gen Loss : 29.09457015991211, \t Total Dis Loss : 0.0005149446660652757\n",
      "Steps : 8200, \t Total Gen Loss : 30.597379684448242, \t Total Dis Loss : 0.0019411456305533648\n",
      "Steps : 8300, \t Total Gen Loss : 30.62472915649414, \t Total Dis Loss : 0.001306689577177167\n",
      "Steps : 8400, \t Total Gen Loss : 30.166879653930664, \t Total Dis Loss : 0.0008720721234567463\n",
      "Steps : 8500, \t Total Gen Loss : 29.265308380126953, \t Total Dis Loss : 0.003453500336036086\n",
      "Steps : 8600, \t Total Gen Loss : 35.30208206176758, \t Total Dis Loss : 0.0035151743795722723\n",
      "Steps : 8700, \t Total Gen Loss : 32.2912483215332, \t Total Dis Loss : 0.004398334305733442\n",
      "Steps : 8800, \t Total Gen Loss : 30.47429847717285, \t Total Dis Loss : 0.026716232299804688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 8900, \t Total Gen Loss : 30.512435913085938, \t Total Dis Loss : 0.002702775876969099\n",
      "Steps : 9000, \t Total Gen Loss : 31.003610610961914, \t Total Dis Loss : 0.0011082467390224338\n",
      "Steps : 9100, \t Total Gen Loss : 32.6804313659668, \t Total Dis Loss : 0.010673555545508862\n",
      "Steps : 9200, \t Total Gen Loss : 31.426454544067383, \t Total Dis Loss : 0.0011484276037663221\n",
      "Steps : 9300, \t Total Gen Loss : 30.953262329101562, \t Total Dis Loss : 0.0016151982126757503\n",
      "Steps : 9400, \t Total Gen Loss : 33.10356521606445, \t Total Dis Loss : 0.0015854556113481522\n",
      "Steps : 9500, \t Total Gen Loss : 31.494014739990234, \t Total Dis Loss : 0.003380544250831008\n",
      "Steps : 9600, \t Total Gen Loss : 30.551239013671875, \t Total Dis Loss : 0.4987736940383911\n",
      "Steps : 9700, \t Total Gen Loss : 29.30673599243164, \t Total Dis Loss : 0.003581955563277006\n",
      "Steps : 9800, \t Total Gen Loss : 29.60118865966797, \t Total Dis Loss : 0.0073826927691698074\n",
      "Steps : 9900, \t Total Gen Loss : 31.713333129882812, \t Total Dis Loss : 0.0012897445121780038\n",
      "Steps : 10000, \t Total Gen Loss : 31.533584594726562, \t Total Dis Loss : 0.0010062232613563538\n",
      "Steps : 10100, \t Total Gen Loss : 31.02142333984375, \t Total Dis Loss : 0.0004192887572571635\n",
      "Steps : 10200, \t Total Gen Loss : 29.47115135192871, \t Total Dis Loss : 0.00156793009955436\n",
      "Steps : 10300, \t Total Gen Loss : 34.53466796875, \t Total Dis Loss : 0.004915020428597927\n",
      "Steps : 10400, \t Total Gen Loss : 32.540706634521484, \t Total Dis Loss : 0.0006411299691535532\n",
      "Steps : 10500, \t Total Gen Loss : 32.72309494018555, \t Total Dis Loss : 0.0010559569345787168\n",
      "Steps : 10600, \t Total Gen Loss : 31.32773208618164, \t Total Dis Loss : 0.00257663382217288\n",
      "Steps : 10700, \t Total Gen Loss : 31.7801513671875, \t Total Dis Loss : 0.000593527453020215\n",
      "Steps : 10800, \t Total Gen Loss : 31.63202667236328, \t Total Dis Loss : 0.000940605707000941\n",
      "Steps : 10900, \t Total Gen Loss : 32.427162170410156, \t Total Dis Loss : 0.00040414539398625493\n",
      "Steps : 11000, \t Total Gen Loss : 30.336206436157227, \t Total Dis Loss : 0.0004477409238461405\n",
      "Steps : 11100, \t Total Gen Loss : 31.796335220336914, \t Total Dis Loss : 0.0013794138794764876\n",
      "Steps : 11200, \t Total Gen Loss : 31.777202606201172, \t Total Dis Loss : 0.0005613660323433578\n",
      "Steps : 11300, \t Total Gen Loss : 33.60040283203125, \t Total Dis Loss : 0.0010229205945506692\n",
      "Steps : 11400, \t Total Gen Loss : 32.66885757446289, \t Total Dis Loss : 0.0006934035336598754\n",
      "Steps : 11500, \t Total Gen Loss : 33.24130630493164, \t Total Dis Loss : 0.0017922519473358989\n",
      "Steps : 11600, \t Total Gen Loss : 32.32662582397461, \t Total Dis Loss : 0.016103027388453484\n",
      "Steps : 11700, \t Total Gen Loss : 32.987274169921875, \t Total Dis Loss : 0.00045435826177708805\n",
      "Steps : 11800, \t Total Gen Loss : 32.5628547668457, \t Total Dis Loss : 0.0025913603603839874\n",
      "Steps : 11900, \t Total Gen Loss : 32.37498092651367, \t Total Dis Loss : 0.0008029314922168851\n",
      "Steps : 12000, \t Total Gen Loss : 32.203983306884766, \t Total Dis Loss : 0.00047192699275910854\n",
      "Steps : 12100, \t Total Gen Loss : 33.824466705322266, \t Total Dis Loss : 0.00026865664403885603\n",
      "Steps : 12200, \t Total Gen Loss : 33.520729064941406, \t Total Dis Loss : 0.00030813319608569145\n",
      "Steps : 12300, \t Total Gen Loss : 30.837364196777344, \t Total Dis Loss : 0.0007679053233005106\n",
      "Steps : 12400, \t Total Gen Loss : 28.797269821166992, \t Total Dis Loss : 0.010381007567048073\n",
      "Steps : 12500, \t Total Gen Loss : 28.544069290161133, \t Total Dis Loss : 0.06742530316114426\n",
      "Steps : 12600, \t Total Gen Loss : 31.123390197753906, \t Total Dis Loss : 0.07585430890321732\n",
      "Steps : 12700, \t Total Gen Loss : 34.211517333984375, \t Total Dis Loss : 0.0011054277420043945\n",
      "Steps : 12800, \t Total Gen Loss : 33.59828186035156, \t Total Dis Loss : 0.0005517419194802642\n",
      "Steps : 12900, \t Total Gen Loss : 29.842031478881836, \t Total Dis Loss : 0.0012988452799618244\n",
      "Steps : 13000, \t Total Gen Loss : 29.74757957458496, \t Total Dis Loss : 0.0007943602977320552\n",
      "Steps : 13100, \t Total Gen Loss : 31.505218505859375, \t Total Dis Loss : 0.0008285909425467253\n",
      "Steps : 13200, \t Total Gen Loss : 31.411344528198242, \t Total Dis Loss : 0.00041112693725153804\n",
      "Steps : 13300, \t Total Gen Loss : 32.690067291259766, \t Total Dis Loss : 0.000363874773029238\n",
      "Steps : 13400, \t Total Gen Loss : 30.316965103149414, \t Total Dis Loss : 0.0028232650365680456\n",
      "Steps : 13500, \t Total Gen Loss : 29.421924591064453, \t Total Dis Loss : 0.0008561454596929252\n",
      "Time for epoch 2 is 415.66268658638 sec\n",
      "Steps : 13600, \t Total Gen Loss : 32.66914749145508, \t Total Dis Loss : 0.0009430174250155687\n",
      "Steps : 13700, \t Total Gen Loss : 30.640350341796875, \t Total Dis Loss : 0.0010697162942960858\n",
      "Steps : 13800, \t Total Gen Loss : 31.36281967163086, \t Total Dis Loss : 0.0019665248692035675\n",
      "Steps : 13900, \t Total Gen Loss : 31.46234130859375, \t Total Dis Loss : 0.00048547491314820945\n",
      "Steps : 14000, \t Total Gen Loss : 31.4119815826416, \t Total Dis Loss : 0.013182354159653187\n",
      "Steps : 14100, \t Total Gen Loss : 30.29207992553711, \t Total Dis Loss : 0.0009664802346378565\n",
      "Steps : 14200, \t Total Gen Loss : 32.82516860961914, \t Total Dis Loss : 0.20713740587234497\n",
      "Steps : 14300, \t Total Gen Loss : 33.23968505859375, \t Total Dis Loss : 0.00036499870475381613\n",
      "Steps : 14400, \t Total Gen Loss : 29.121980667114258, \t Total Dis Loss : 0.0031495948787778616\n",
      "Steps : 14500, \t Total Gen Loss : 33.288475036621094, \t Total Dis Loss : 0.012430178932845592\n",
      "Steps : 14600, \t Total Gen Loss : 32.081844329833984, \t Total Dis Loss : 0.0006761723780073225\n",
      "Steps : 14700, \t Total Gen Loss : 32.267269134521484, \t Total Dis Loss : 0.0006937709404155612\n",
      "Steps : 14800, \t Total Gen Loss : 32.95677947998047, \t Total Dis Loss : 0.0005672521074302495\n",
      "Steps : 14900, \t Total Gen Loss : 32.33340835571289, \t Total Dis Loss : 0.0008868383010849357\n",
      "Steps : 15000, \t Total Gen Loss : 31.220163345336914, \t Total Dis Loss : 0.03718451038002968\n",
      "Steps : 15100, \t Total Gen Loss : 30.588542938232422, \t Total Dis Loss : 0.00048592485836707056\n",
      "Steps : 15200, \t Total Gen Loss : 33.605403900146484, \t Total Dis Loss : 0.0008225631318055093\n",
      "Steps : 15300, \t Total Gen Loss : 30.159879684448242, \t Total Dis Loss : 0.0226422268897295\n",
      "Steps : 15400, \t Total Gen Loss : 31.31273078918457, \t Total Dis Loss : 0.0017940399702638388\n",
      "Steps : 15500, \t Total Gen Loss : 34.09903335571289, \t Total Dis Loss : 0.0022589208092540503\n",
      "Steps : 15600, \t Total Gen Loss : 32.884803771972656, \t Total Dis Loss : 0.00046834422391839325\n",
      "Steps : 15700, \t Total Gen Loss : 33.43098831176758, \t Total Dis Loss : 0.0015510384691879153\n",
      "Steps : 15800, \t Total Gen Loss : 31.98180389404297, \t Total Dis Loss : 0.0008777822367846966\n",
      "Steps : 15900, \t Total Gen Loss : 31.529481887817383, \t Total Dis Loss : 0.0008896505460143089\n",
      "Steps : 16000, \t Total Gen Loss : 32.63058853149414, \t Total Dis Loss : 0.0008678692975081503\n",
      "Steps : 16100, \t Total Gen Loss : 31.963411331176758, \t Total Dis Loss : 0.005485696252435446\n",
      "Steps : 16200, \t Total Gen Loss : 31.850019454956055, \t Total Dis Loss : 0.0020336410962045193\n",
      "Steps : 16300, \t Total Gen Loss : 30.097244262695312, \t Total Dis Loss : 0.0022042514756321907\n",
      "Steps : 16400, \t Total Gen Loss : 31.475088119506836, \t Total Dis Loss : 0.0010850841645151377\n",
      "Steps : 16500, \t Total Gen Loss : 33.78916549682617, \t Total Dis Loss : 0.0029628537595272064\n",
      "Steps : 16600, \t Total Gen Loss : 33.09601974487305, \t Total Dis Loss : 0.01138628926128149\n",
      "Steps : 16700, \t Total Gen Loss : 30.57799530029297, \t Total Dis Loss : 0.0016769944922998548\n",
      "Steps : 16800, \t Total Gen Loss : 29.531959533691406, \t Total Dis Loss : 0.003092346014454961\n",
      "Steps : 16900, \t Total Gen Loss : 34.50651550292969, \t Total Dis Loss : 0.0031032611150294542\n",
      "Steps : 17000, \t Total Gen Loss : 33.14053726196289, \t Total Dis Loss : 0.0016497864853590727\n",
      "Steps : 17100, \t Total Gen Loss : 32.52897644042969, \t Total Dis Loss : 0.001385422539897263\n",
      "Steps : 17200, \t Total Gen Loss : 33.62773513793945, \t Total Dis Loss : 0.0016589593142271042\n",
      "Steps : 17300, \t Total Gen Loss : 34.11790466308594, \t Total Dis Loss : 0.002166459569707513\n",
      "Steps : 17400, \t Total Gen Loss : 34.98400115966797, \t Total Dis Loss : 0.0003534065617714077\n",
      "Steps : 17500, \t Total Gen Loss : 33.44154357910156, \t Total Dis Loss : 0.008351421914994717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 17600, \t Total Gen Loss : 33.64118194580078, \t Total Dis Loss : 0.000619197846390307\n",
      "Steps : 17700, \t Total Gen Loss : 33.0833740234375, \t Total Dis Loss : 0.0004977376665920019\n",
      "Steps : 17800, \t Total Gen Loss : 33.34765625, \t Total Dis Loss : 0.00020299605967011303\n",
      "Steps : 17900, \t Total Gen Loss : 34.220550537109375, \t Total Dis Loss : 0.00015708488353993744\n",
      "Steps : 18000, \t Total Gen Loss : 30.576940536499023, \t Total Dis Loss : 0.002053576521575451\n",
      "Steps : 18100, \t Total Gen Loss : 33.34306335449219, \t Total Dis Loss : 0.0003183399676345289\n",
      "Steps : 18200, \t Total Gen Loss : 30.510961532592773, \t Total Dis Loss : 0.00024368250160478055\n",
      "Steps : 18300, \t Total Gen Loss : 31.188495635986328, \t Total Dis Loss : 0.0034905087668448687\n",
      "Steps : 18400, \t Total Gen Loss : 28.313634872436523, \t Total Dis Loss : 0.001223771134391427\n",
      "Steps : 18500, \t Total Gen Loss : 34.355037689208984, \t Total Dis Loss : 0.0006141242920421064\n",
      "Steps : 18600, \t Total Gen Loss : 31.87984275817871, \t Total Dis Loss : 0.0013118641218170524\n",
      "Steps : 18700, \t Total Gen Loss : 33.39249038696289, \t Total Dis Loss : 0.005049359053373337\n",
      "Steps : 18800, \t Total Gen Loss : 34.74402618408203, \t Total Dis Loss : 0.00039234088035300374\n",
      "Steps : 18900, \t Total Gen Loss : 30.974353790283203, \t Total Dis Loss : 0.0005921269184909761\n",
      "Steps : 19000, \t Total Gen Loss : 31.347625732421875, \t Total Dis Loss : 0.001984698697924614\n",
      "Steps : 19100, \t Total Gen Loss : 33.76249694824219, \t Total Dis Loss : 0.00112646643538028\n",
      "Steps : 19200, \t Total Gen Loss : 31.753189086914062, \t Total Dis Loss : 0.0015811121556907892\n",
      "Steps : 19300, \t Total Gen Loss : 32.35082244873047, \t Total Dis Loss : 0.013953239656984806\n",
      "Steps : 19400, \t Total Gen Loss : 33.657501220703125, \t Total Dis Loss : 0.0010806419886648655\n",
      "Steps : 19500, \t Total Gen Loss : 32.15983963012695, \t Total Dis Loss : 0.0015084098558872938\n",
      "Steps : 19600, \t Total Gen Loss : 32.595123291015625, \t Total Dis Loss : 0.0009366705198772252\n",
      "Steps : 19700, \t Total Gen Loss : 29.635433197021484, \t Total Dis Loss : 0.0017566618043929338\n",
      "Steps : 19800, \t Total Gen Loss : 32.3234977722168, \t Total Dis Loss : 0.0012805636506527662\n",
      "Steps : 19900, \t Total Gen Loss : 31.537145614624023, \t Total Dis Loss : 0.0002629532536957413\n",
      "Steps : 20000, \t Total Gen Loss : 34.45281982421875, \t Total Dis Loss : 0.00036048275069333613\n",
      "Steps : 20100, \t Total Gen Loss : 32.823631286621094, \t Total Dis Loss : 0.001460685976780951\n",
      "Steps : 20200, \t Total Gen Loss : 33.55946350097656, \t Total Dis Loss : 0.002113457303494215\n",
      "Time for epoch 3 is 414.9776759147644 sec\n",
      "Steps : 20300, \t Total Gen Loss : 35.767826080322266, \t Total Dis Loss : 0.0004052963340654969\n",
      "Steps : 20400, \t Total Gen Loss : 32.65972137451172, \t Total Dis Loss : 0.0017642354359850287\n",
      "Steps : 20500, \t Total Gen Loss : 33.87649154663086, \t Total Dis Loss : 0.0004959544166922569\n",
      "Steps : 20600, \t Total Gen Loss : 31.58049201965332, \t Total Dis Loss : 0.0004944975953549147\n",
      "Steps : 20700, \t Total Gen Loss : 31.59515953063965, \t Total Dis Loss : 0.0005220199236646295\n",
      "Steps : 20800, \t Total Gen Loss : 34.7749137878418, \t Total Dis Loss : 0.0009075384587049484\n",
      "Steps : 20900, \t Total Gen Loss : 34.40763854980469, \t Total Dis Loss : 0.0054053496569395065\n",
      "Steps : 21000, \t Total Gen Loss : 33.51608657836914, \t Total Dis Loss : 0.001446378417313099\n",
      "Steps : 21100, \t Total Gen Loss : 30.208539962768555, \t Total Dis Loss : 0.0006495058769360185\n",
      "Steps : 21200, \t Total Gen Loss : 33.217987060546875, \t Total Dis Loss : 0.0002070491755148396\n",
      "Steps : 21300, \t Total Gen Loss : 31.30156898498535, \t Total Dis Loss : 0.00400471081957221\n",
      "Steps : 21400, \t Total Gen Loss : 33.3453483581543, \t Total Dis Loss : 0.0004731009539682418\n",
      "Steps : 21500, \t Total Gen Loss : 30.484935760498047, \t Total Dis Loss : 0.0011217943392693996\n",
      "Steps : 21600, \t Total Gen Loss : 32.31327438354492, \t Total Dis Loss : 0.00021949506481178105\n",
      "Steps : 21700, \t Total Gen Loss : 34.5588493347168, \t Total Dis Loss : 0.0003543523489497602\n",
      "Steps : 21800, \t Total Gen Loss : 32.140193939208984, \t Total Dis Loss : 0.4059160649776459\n",
      "Steps : 21900, \t Total Gen Loss : 28.761119842529297, \t Total Dis Loss : 0.022690171375870705\n",
      "Steps : 22000, \t Total Gen Loss : 33.32554626464844, \t Total Dis Loss : 0.0007349271909333766\n",
      "Steps : 22100, \t Total Gen Loss : 29.080585479736328, \t Total Dis Loss : 0.0018003876321017742\n",
      "Steps : 22200, \t Total Gen Loss : 28.412446975708008, \t Total Dis Loss : 0.508476734161377\n",
      "Steps : 22300, \t Total Gen Loss : 31.90729331970215, \t Total Dis Loss : 0.007825592532753944\n",
      "Steps : 22400, \t Total Gen Loss : 30.627254486083984, \t Total Dis Loss : 0.0213811956346035\n",
      "Steps : 22500, \t Total Gen Loss : 34.62495803833008, \t Total Dis Loss : 0.0006645453395321965\n",
      "Steps : 22600, \t Total Gen Loss : 34.20191955566406, \t Total Dis Loss : 0.0013067590771242976\n",
      "Steps : 22700, \t Total Gen Loss : 31.686487197875977, \t Total Dis Loss : 0.0007505780085921288\n",
      "Steps : 22800, \t Total Gen Loss : 33.04293441772461, \t Total Dis Loss : 0.029940204694867134\n",
      "Steps : 22900, \t Total Gen Loss : 31.308420181274414, \t Total Dis Loss : 0.010007640346884727\n",
      "Steps : 23000, \t Total Gen Loss : 34.38136672973633, \t Total Dis Loss : 0.012448013760149479\n",
      "Steps : 23100, \t Total Gen Loss : 32.578128814697266, \t Total Dis Loss : 0.00478229159489274\n",
      "Steps : 23200, \t Total Gen Loss : 34.45359420776367, \t Total Dis Loss : 0.00027219337061978877\n",
      "Steps : 23300, \t Total Gen Loss : 34.83699417114258, \t Total Dis Loss : 0.0007630857871845365\n",
      "Steps : 23400, \t Total Gen Loss : 32.73792266845703, \t Total Dis Loss : 0.0014452253235504031\n",
      "Steps : 23500, \t Total Gen Loss : 32.45006561279297, \t Total Dis Loss : 0.00020512483024504036\n",
      "Steps : 23600, \t Total Gen Loss : 32.806575775146484, \t Total Dis Loss : 0.0064685228280723095\n",
      "Steps : 23700, \t Total Gen Loss : 33.6160774230957, \t Total Dis Loss : 0.000486033852212131\n",
      "Steps : 23800, \t Total Gen Loss : 32.65461349487305, \t Total Dis Loss : 0.00017739764007274061\n",
      "Steps : 23900, \t Total Gen Loss : 32.12229537963867, \t Total Dis Loss : 0.00048613580293022096\n",
      "Steps : 24000, \t Total Gen Loss : 37.12519836425781, \t Total Dis Loss : 0.0004893291043117642\n",
      "Steps : 24100, \t Total Gen Loss : 32.38919448852539, \t Total Dis Loss : 0.0004535456537269056\n",
      "Steps : 24200, \t Total Gen Loss : 31.564403533935547, \t Total Dis Loss : 0.0018279849318787456\n",
      "Steps : 24300, \t Total Gen Loss : 34.3035888671875, \t Total Dis Loss : 0.0002922117128036916\n",
      "Steps : 24400, \t Total Gen Loss : 33.25584411621094, \t Total Dis Loss : 0.0002764223318081349\n",
      "Steps : 24500, \t Total Gen Loss : 34.04816818237305, \t Total Dis Loss : 0.003602218348532915\n",
      "Steps : 24600, \t Total Gen Loss : 34.1307373046875, \t Total Dis Loss : 0.000864996574819088\n",
      "Steps : 24700, \t Total Gen Loss : 33.820186614990234, \t Total Dis Loss : 0.0003491840325295925\n",
      "Steps : 24800, \t Total Gen Loss : 32.622154235839844, \t Total Dis Loss : 0.0005906920414417982\n",
      "Steps : 24900, \t Total Gen Loss : 32.4984130859375, \t Total Dis Loss : 0.0007098070345818996\n",
      "Steps : 25000, \t Total Gen Loss : 31.923566818237305, \t Total Dis Loss : 0.0007269641500897706\n",
      "Steps : 25100, \t Total Gen Loss : 31.92679214477539, \t Total Dis Loss : 0.0017535805236548185\n",
      "Steps : 25200, \t Total Gen Loss : 30.654264450073242, \t Total Dis Loss : 0.0007493157754652202\n",
      "Steps : 25300, \t Total Gen Loss : 30.40553092956543, \t Total Dis Loss : 0.0013288967311382294\n",
      "Steps : 25400, \t Total Gen Loss : 32.03450012207031, \t Total Dis Loss : 0.0016627504955977201\n",
      "Steps : 25500, \t Total Gen Loss : 33.882930755615234, \t Total Dis Loss : 0.00017277372535318136\n",
      "Steps : 25600, \t Total Gen Loss : 32.601951599121094, \t Total Dis Loss : 0.0010096951154991984\n",
      "Steps : 25700, \t Total Gen Loss : 35.455833435058594, \t Total Dis Loss : 0.00022831483511254191\n",
      "Steps : 25800, \t Total Gen Loss : 35.25178146362305, \t Total Dis Loss : 0.00017698065494187176\n",
      "Steps : 25900, \t Total Gen Loss : 33.54623794555664, \t Total Dis Loss : 0.0003324946737848222\n",
      "Steps : 26000, \t Total Gen Loss : 33.801212310791016, \t Total Dis Loss : 0.00022395547421183437\n",
      "Steps : 26100, \t Total Gen Loss : 33.4852294921875, \t Total Dis Loss : 0.002075175754725933\n",
      "Steps : 26200, \t Total Gen Loss : 36.20042419433594, \t Total Dis Loss : 0.0005209832452237606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 26300, \t Total Gen Loss : 34.4869270324707, \t Total Dis Loss : 0.00026440160581842065\n",
      "Steps : 26400, \t Total Gen Loss : 31.448965072631836, \t Total Dis Loss : 0.0021524387411773205\n",
      "Steps : 26500, \t Total Gen Loss : 30.674741744995117, \t Total Dis Loss : 0.001356750726699829\n",
      "Steps : 26600, \t Total Gen Loss : 36.5028190612793, \t Total Dis Loss : 0.0001673072256380692\n",
      "Steps : 26700, \t Total Gen Loss : 32.75950241088867, \t Total Dis Loss : 0.0009160332265309989\n",
      "Steps : 26800, \t Total Gen Loss : 31.204092025756836, \t Total Dis Loss : 0.0010738573037087917\n",
      "Steps : 26900, \t Total Gen Loss : 32.69697952270508, \t Total Dis Loss : 0.00020433431200217456\n",
      "Steps : 27000, \t Total Gen Loss : 32.84028625488281, \t Total Dis Loss : 0.00045467138988897204\n",
      "Time for epoch 4 is 416.6502101421356 sec\n",
      "Steps : 27100, \t Total Gen Loss : 32.694454193115234, \t Total Dis Loss : 0.000487496901769191\n",
      "Steps : 27200, \t Total Gen Loss : 33.57573699951172, \t Total Dis Loss : 0.000326937937643379\n",
      "Steps : 27300, \t Total Gen Loss : 31.83255386352539, \t Total Dis Loss : 0.0006006341427564621\n",
      "Steps : 27400, \t Total Gen Loss : 33.524227142333984, \t Total Dis Loss : 0.0006532216793857515\n",
      "Steps : 27500, \t Total Gen Loss : 35.69450759887695, \t Total Dis Loss : 0.00017122244753409177\n",
      "Steps : 27600, \t Total Gen Loss : 32.26007080078125, \t Total Dis Loss : 0.00538895046338439\n",
      "Steps : 27700, \t Total Gen Loss : 33.8360710144043, \t Total Dis Loss : 0.004942086059600115\n",
      "Steps : 27800, \t Total Gen Loss : 32.80121612548828, \t Total Dis Loss : 0.0006623810040764511\n",
      "Steps : 27900, \t Total Gen Loss : 34.971649169921875, \t Total Dis Loss : 0.001079905079677701\n",
      "Steps : 28000, \t Total Gen Loss : 33.725669860839844, \t Total Dis Loss : 0.00043687238940037787\n",
      "Steps : 28100, \t Total Gen Loss : 32.92012023925781, \t Total Dis Loss : 0.004944080952554941\n",
      "Steps : 28200, \t Total Gen Loss : 33.476356506347656, \t Total Dis Loss : 0.00549876457080245\n",
      "Steps : 28300, \t Total Gen Loss : 35.7575569152832, \t Total Dis Loss : 0.00010997027129633352\n",
      "Steps : 28400, \t Total Gen Loss : 34.28567123413086, \t Total Dis Loss : 0.001221830490976572\n",
      "Steps : 28500, \t Total Gen Loss : 30.55718231201172, \t Total Dis Loss : 0.001329605933278799\n",
      "Steps : 28600, \t Total Gen Loss : 33.591487884521484, \t Total Dis Loss : 0.0004590569587890059\n",
      "Steps : 28700, \t Total Gen Loss : 32.940643310546875, \t Total Dis Loss : 0.000537745887413621\n",
      "Steps : 28800, \t Total Gen Loss : 35.52865219116211, \t Total Dis Loss : 0.0008829753496684134\n",
      "Steps : 28900, \t Total Gen Loss : 32.5042610168457, \t Total Dis Loss : 0.0003192902950104326\n",
      "Steps : 29000, \t Total Gen Loss : 34.618492126464844, \t Total Dis Loss : 0.00021088030189275742\n",
      "Steps : 29100, \t Total Gen Loss : 34.43022918701172, \t Total Dis Loss : 0.006366997491568327\n",
      "Steps : 29200, \t Total Gen Loss : 33.97932815551758, \t Total Dis Loss : 0.0003330394101794809\n",
      "Steps : 29300, \t Total Gen Loss : 31.288829803466797, \t Total Dis Loss : 0.0003424656460992992\n",
      "Steps : 29400, \t Total Gen Loss : 32.373470306396484, \t Total Dis Loss : 0.0008270315011031926\n",
      "Steps : 29500, \t Total Gen Loss : 30.849285125732422, \t Total Dis Loss : 0.0005606670747511089\n",
      "Steps : 29600, \t Total Gen Loss : 33.03834533691406, \t Total Dis Loss : 0.0009698588401079178\n",
      "Steps : 29700, \t Total Gen Loss : 30.97648811340332, \t Total Dis Loss : 0.0008026961586438119\n",
      "Steps : 29800, \t Total Gen Loss : 33.4222412109375, \t Total Dis Loss : 0.001139021827839315\n",
      "Steps : 29900, \t Total Gen Loss : 31.78774642944336, \t Total Dis Loss : 0.0014843326061964035\n",
      "Steps : 30000, \t Total Gen Loss : 33.62694549560547, \t Total Dis Loss : 0.0006421389989554882\n",
      "Steps : 30100, \t Total Gen Loss : 29.959543228149414, \t Total Dis Loss : 0.0006316946819424629\n",
      "Steps : 30200, \t Total Gen Loss : 32.91532516479492, \t Total Dis Loss : 0.00016990816220641136\n",
      "Steps : 30300, \t Total Gen Loss : 28.856990814208984, \t Total Dis Loss : 0.11691536009311676\n",
      "Steps : 30400, \t Total Gen Loss : 30.21791648864746, \t Total Dis Loss : 0.0003759285027626902\n",
      "Steps : 30500, \t Total Gen Loss : 29.309301376342773, \t Total Dis Loss : 0.0011456195497885346\n",
      "Steps : 30600, \t Total Gen Loss : 33.92607116699219, \t Total Dis Loss : 0.002142771612852812\n",
      "Steps : 30700, \t Total Gen Loss : 32.9437255859375, \t Total Dis Loss : 0.002063304651528597\n",
      "Steps : 30800, \t Total Gen Loss : 31.999496459960938, \t Total Dis Loss : 0.0003828348417300731\n",
      "Steps : 30900, \t Total Gen Loss : 33.597747802734375, \t Total Dis Loss : 0.0001954072795342654\n",
      "Steps : 31000, \t Total Gen Loss : 31.736852645874023, \t Total Dis Loss : 0.002406798768788576\n",
      "Steps : 31100, \t Total Gen Loss : 33.84357452392578, \t Total Dis Loss : 0.00033688495750539005\n",
      "Steps : 31200, \t Total Gen Loss : 33.515663146972656, \t Total Dis Loss : 9.241653606295586e-05\n",
      "Steps : 31300, \t Total Gen Loss : 31.720718383789062, \t Total Dis Loss : 0.0008015845669433475\n",
      "Steps : 31400, \t Total Gen Loss : 34.1065673828125, \t Total Dis Loss : 0.0002573218080215156\n",
      "Steps : 31500, \t Total Gen Loss : 32.547264099121094, \t Total Dis Loss : 0.0005812523304484785\n",
      "Steps : 31600, \t Total Gen Loss : 30.512378692626953, \t Total Dis Loss : 0.002564874943345785\n",
      "Steps : 31700, \t Total Gen Loss : 28.816089630126953, \t Total Dis Loss : 0.001536215771920979\n",
      "Steps : 31800, \t Total Gen Loss : 29.684175491333008, \t Total Dis Loss : 0.0005130096687935293\n",
      "Steps : 31900, \t Total Gen Loss : 30.441936492919922, \t Total Dis Loss : 0.0012568014208227396\n",
      "Steps : 32000, \t Total Gen Loss : 29.395933151245117, \t Total Dis Loss : 0.0006051243981346488\n",
      "Steps : 32100, \t Total Gen Loss : 33.21788787841797, \t Total Dis Loss : 0.0004988181754015386\n",
      "Steps : 32200, \t Total Gen Loss : 31.945606231689453, \t Total Dis Loss : 0.00033230031840503216\n",
      "Steps : 32300, \t Total Gen Loss : 31.954917907714844, \t Total Dis Loss : 0.0003580941120162606\n",
      "Steps : 32400, \t Total Gen Loss : 31.579694747924805, \t Total Dis Loss : 0.00016848341329023242\n",
      "Steps : 32500, \t Total Gen Loss : 32.900970458984375, \t Total Dis Loss : 0.00011441591050243005\n",
      "Steps : 32600, \t Total Gen Loss : 32.658966064453125, \t Total Dis Loss : 0.0006032748497091234\n",
      "Steps : 32700, \t Total Gen Loss : 30.26686668395996, \t Total Dis Loss : 0.001153786201030016\n",
      "Steps : 32800, \t Total Gen Loss : 31.891834259033203, \t Total Dis Loss : 0.00018143723718822002\n",
      "Steps : 32900, \t Total Gen Loss : 36.825260162353516, \t Total Dis Loss : 0.0004045061068609357\n",
      "Steps : 33000, \t Total Gen Loss : 33.77399826049805, \t Total Dis Loss : 0.00040641563828103244\n",
      "Steps : 33100, \t Total Gen Loss : 35.12828826904297, \t Total Dis Loss : 0.00013131169544067234\n",
      "Steps : 33200, \t Total Gen Loss : 35.044403076171875, \t Total Dis Loss : 2.8719017791445367e-05\n",
      "Steps : 33300, \t Total Gen Loss : 32.82917404174805, \t Total Dis Loss : 7.025853119557723e-05\n",
      "Steps : 33400, \t Total Gen Loss : 32.591495513916016, \t Total Dis Loss : 0.0004295790567994118\n",
      "Steps : 33500, \t Total Gen Loss : 32.4903678894043, \t Total Dis Loss : 0.0010112342424690723\n",
      "Steps : 33600, \t Total Gen Loss : 31.389610290527344, \t Total Dis Loss : 0.048895951360464096\n",
      "Steps : 33700, \t Total Gen Loss : 34.69602966308594, \t Total Dis Loss : 0.0002183851902373135\n",
      "Time for epoch 5 is 413.0996980667114 sec\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 5\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7fdba208dad0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 16000\n"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4865234 0.47190255\n",
      "0.1448972 0.14040434\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이상감지용 데이터셋 구축 (개구리 데이터를 학습데이터셋에서 제외하여 테스트 데이터셋에 포함)\n",
    "- Skip-GANomaly 모델의 구현\n",
    "- 모델의 학습과 검증\n",
    "- 검증 결과의 시각화 (정상-이상 데이터의 anomaly score 분포 시각화, 적절한 threshold에 따른 이삼감지율 계-산, 감지 성공/실패사례 시각화 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
