{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1. 형태소 분석기 변경해 보기\n",
    "\n",
    " - 참고 : https://konlpy.org/ko/v0.5.2/api/konlpy.tag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#start = time.time()  # 시작 시간 저장\n",
    "#print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['밤', '에', '귀가', '하', '던', '여성', '에게', '범죄', '를', '시도', '한', '대', '남성', '이', '구속', '됐', '다', '서울', '제주', '경찰서', '는', '상해', '혐의', '로', '씨', '를', '구속', '해', '수사', '하', '고', '있', '다고', '일', '밝혔', '다', '씨', '는', '지난달', '일', '피해', '여성', '을', '인근', '지하철', '역', '에서부터', '따라가', '폭행', '을', '시도', '하', '려다가', '도망간', '혐의', '를', '받', '는다', '피해', '여성', '이', '저항', '하', '자', '놀란', '씨', '는', '도망갔으며', '신고', '를', '받', '고', '주변', '을', '수색', '하', '던', '경찰', '에', '체포', '됐', '다', '피해', '여성', '은', '이', '과정', '에서', '경미', '한', '부상', '을', '입', '은', '것', '으로', '전해졌', '다']\n",
      "Mecab time : 0.0013935565948486328\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kor_text = '밤에 귀가하던 여성에게 범죄를 시도한 대 남성이 구속됐다서울 제주경찰서는 \\\n",
    "            상해 혐의로 씨를 구속해 수사하고 있다고 일 밝혔다씨는 지난달 일 피해 여성을 \\\n",
    "            인근 지하철 역에서부터 따라가 폭행을 시도하려다가 도망간 혐의를 받는다피해 \\\n",
    "            여성이 저항하자 놀란 씨는 도망갔으며 신고를 받고 주변을 수색하던 경찰에 \\\n",
    "            체포됐다피해 여성은 이 과정에서 경미한 부상을 입은 것으로 전해졌다'\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer_Mecab = Mecab()\n",
    "\n",
    "\n",
    "#- 형태소 분석, 즉 토큰화(tokenization)를 합니다.\n",
    "start = time.time()  # 시작 시간 저장\n",
    "print(tokenizer_Mecab.morphs(kor_text))\n",
    "print(\"Mecab time :\", time.time() - start) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-11  프로젝트를 하기 위해서는 konlpy를 이용해야 하는데 그대로 실행할 경우 java가 없어서 에러가 납니다.\n",
    "https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-on-ubuntu-18-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['밤', '에', '귀가', '하', '던', '여성', '에게', '범죄', '를', '시도', '하', 'ㄴ', '대', '남성', '이', '구속됐다서울', '제주경찰', '서는', '상하', '어', '혐의', '로', '씨', '를', '구속해', '수사', '하고', '있', '다', '고', '일', '밝혔다씨', '는', '지난달', '일', '피하', '어', '여성', '을', '인근', '지하철', '역', '에서부터', '따르', '아', '가', '아', '폭행', '을', '시도', '하', '려', '다가', '도망가', 'ㄴ', '혐의', '를', '받는다피해', '여성', '이', '저항', '하', '자', '놀라', 'ㄴ', '씨', '는', '도망가', '아며', '신고', '를', '받', '고', '주변', '을', '수색', '하', '던', '경찰', '에', '체포됐다피해', '여성', '은', '이', '과정', '에서', '경미한', '부상', '을', '입', '은', '것', '으로', '전하', '어', '지', '었다']\n",
      "Hannanum time : 0.48087286949157715\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "\n",
    "tokenizer_Hannanum = Hannanum()\n",
    "\n",
    "#- 형태소 분석, 즉 토큰화(tokenization)를 합니다.\n",
    "start = time.time()\n",
    "print(tokenizer_Hannanum.morphs(kor_text))\n",
    "print(\"Hannanum time :\", time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['밤', '에', '귀가', '하', '더', 'ㄴ', '여성', '에게', '범죄', '를', '시도', '하', 'ㄴ', '대', '남성', '이', '구속', '되', '었', '다', '서울', '제주', '경찰서', '는', '상해', '혐의', '로', '씨', '를', '구속', '하', '어', '수사', '하', '고', '있', '다고', '일', '밝히', '었', '다', '씨', '는', '지난달', '일', '피해', '여성', '을', '인근', '지하철', '역', '에서', '부터', '따라가', '폭행을', '시도', '하', '려', '다그', '아', '도망가', 'ㄴ', '혐의', '를', '받', '는', '다', '피해', '여성', '이', '저항', '하', '자', '놀라', 'ㄴ', '씨', '는', '도망가', '었', '으며', '신고', '를', '받', '고', '주변', '을', '수색', '하', '더', 'ㄴ', '경찰', '에', '체포', '되', '었', '다', '피해', '여성', '은', '이', '과정', '에서', '경미', '하', 'ㄴ', '부상', '을', '입', '은', '것', '으로', '전하', '어', '지', '었', '다']\n",
      "Kkma time : 0.24608254432678223\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "tokenizer_Kkma = Kkma()\n",
    "\n",
    "#- 형태소 분석, 즉 토큰화(tokenization)를 합니다.\n",
    "start = time.time()\n",
    "print(tokenizer_Kkma.morphs(kor_text))\n",
    "print(\"Kkma time :\", time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['밤', '에', '귀가', '하', '던', '여성', '에게', '범죄', '를', '시도', '하', 'ㄴ', '대', '남성', '이', '구속', '되', '었', '다', '서울', '제주', '경찰서', '는', '상해', '혐의', '로', '씨', '를', '구속', '하', '아', '수사', '하', '고', '있', '다고', '일', '밝히', '었', '다', '씨', '는', '지난달', '일', '피해', '여성', '을', '인근', '지하철', '역', '에서부터', '따라가', '아', '폭행', '을', '시도', '하', '려다가', '도망가', 'ㄴ', '혐의', '를', '받', '는다', '피하', '아', '여성', '이', '저항', '하', '자', '놀라', 'ㄴ', '씨', '는', '도망가', '았', '으며', '신고', '를', '받', '고', '주변', '을', '수색', '하', '던', '경찰', '에', '체포', '되', '었', '다', '피하', '아', '여성', '은', '이', '과정', '에서', '경미', '하', 'ㄴ', '부상', '을', '입', '은', '것', '으로', '전하', '아', '지', '었', '다']\n",
      "Komoran time : 0.0075702667236328125\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "tokenizer_Komoran = Komoran()\n",
    "\n",
    "start = time.time()\n",
    "print(tokenizer_Komoran.morphs(kor_text))\n",
    "print(\"Komoran time :\", time.time() - start) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석 결과\n",
    "\n",
    "### Mecab을 쓰기로 결정\n",
    "\n",
    "아래의 파일에 알고리즘별 분석 결과\n",
    "속도와 품질 모두를 고려했을 경우, Mecab이 가장 효율적인 것으로 판명되었다.\n",
    "\n",
    "/home/aiffel/Desktop/Changhee/changhee_git/Web Crawling/comparing tokenizer algorithm.ods\n",
    "\n",
    "\n",
    "\n",
    "- 참고 : https://iostream.tistory.com/144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News_data2 가져와서 학습시켜보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 페이지 수, 카테고리, 날짜를 입력값으로 받습니다.\\ndef make_urllist(page_num, code, date): \\n  urllist= []\\n  for i in range(1, page_num + 1):\\n    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)   \\n    news = requests.get(url)\\n\\n    # BeautifulSoup의 인스턴스 생성합니다. 파서는 html.parser를 사용합니다.\\n    soup = BeautifulSoup(news.content, 'html.parser')\\n\\n    # CASE 1\\n    news_list = soup.select('.newsflash_body .type06_headline li dl')\\n    # CASE 2\\n    news_list.extend(soup.select('.newsflash_body .type06 li dl'))\\n        \\n    # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져옵니다.\\n    for line in news_list:\\n        urllist.append(line.a.get('href'))\\n  return urllist\\n  \""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "'''\n",
    "# 페이지 수, 카테고리, 날짜를 입력값으로 받습니다.\n",
    "def make_urllist(page_num, code, date): \n",
    "  urllist= []\n",
    "  for i in range(1, page_num + 1):\n",
    "    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)   \n",
    "    news = requests.get(url)\n",
    "\n",
    "    # BeautifulSoup의 인스턴스 생성합니다. 파서는 html.parser를 사용합니다.\n",
    "    soup = BeautifulSoup(news.content, 'html.parser')\n",
    "\n",
    "    # CASE 1\n",
    "    news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "    # CASE 2\n",
    "    news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "        \n",
    "    # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져옵니다.\n",
    "    for line in news_list:\n",
    "        urllist.append(line.a.get('href'))\n",
    "  return urllist\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "csv_path1 = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
    "csv_path2 = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data2.csv\"\n",
    "\n",
    "df1 = pd.read_table(csv_path1, sep=',')\n",
    "df2 = pd.read_table(csv_path2, sep=',')\n",
    "\n",
    "#정규표현식을 이용해 한글과 영어만 저장함\n",
    "df1['news'] = df1['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "df2['news'] = df2['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "# 중복된 샘플들을 제거합니다.\n",
    "df1.drop_duplicates(subset=['news'], inplace=True)\n",
    "df2.drop_duplicates(subset=['news'], inplace=True)\n",
    "\n",
    "#토큰화 하는 알고리즘\n",
    "tokenizer=Mecab()\n",
    "#불용어 리스트\n",
    "\n",
    "'''\n",
    "stopwords_before=['에','는','은','을','했','에게','있','이','의','하','한','다','과',\n",
    "             '때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가',\n",
    "             '등','들','파이낸셜','저작','등','뉴스]\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "stopwords = ['에','는','은','을','했','에게','있','이','의','하','한','다','과',\n",
    "             '때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가',\n",
    "             '등','들','파이낸셜','저작','등','뉴스','배포', '매일신문', '으로', '에서', '이나', '전경', '사진',\n",
    "             '데', '다고', '도', '로', '뿐', '만', '해', '와', '겠', '달리', '면서', '를', '며', '라는','입니다',\n",
    "             '이미', '면서']\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "    temp_data = []\n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer.morphs(sentence) \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords_2] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data\n",
    "\n",
    "\n",
    "#불용어 추가전 데이터\n",
    "text_data1=preprocessing(df1['news'])\n",
    "\n",
    "\n",
    "#불용어 추가후 데이터\n",
    "text_data_add_nowords1=preprocessing_add_nowords(df1['news'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3994\n",
      "3994\n"
     ]
    }
   ],
   "source": [
    "print(len(text_data1))\n",
    "print(len(text_data_add_nowords1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3994"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(text_data1[0])\n",
    "print(text_data1[10])\n",
    "print(text_data1[50])\n",
    "print(text_data2[100])\n",
    "print(text_data2[200])\n",
    "print(text_data2[300])\n",
    "'''\n",
    "\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus =text_data1\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "sorted(vect.vocabulary_.items(),key=(lambda x:x[1]), reverse=True)\n",
    "'''\n",
    "\n",
    "len(text_data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. 불용어 추가하기\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "print(len(stopwords))\n",
    "# 추가한 불용어\n",
    "#배포, 매일신문, 으로, 에서, 이나, 전경, 사진, 데, 다고, 도, 로, 뿐, 만, 해, 와, 겠, 달리, 면서, 를, 며, 라는,입니다, 이미, 면서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.90      0.80      0.85       200\n",
      "          사회       0.82      0.91      0.86       445\n",
      "       생활/문화       0.85      0.77      0.81       354\n",
      "\n",
      "    accuracy                           0.84       999\n",
      "   macro avg       0.85      0.83      0.84       999\n",
      "weighted avg       0.84      0.84      0.84       999\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.90      0.80      0.85       200\n",
      "          사회       0.82      0.91      0.86       445\n",
      "       생활/문화       0.85      0.77      0.81       354\n",
      "\n",
      "    accuracy                           0.84       999\n",
      "   macro avg       0.85      0.83      0.84       999\n",
      "weighted avg       0.84      0.84      0.84       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#추가전 후 비교\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "#- 불용어 제거 전 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data1, df1['code'], random_state = 1)\n",
    "\n",
    "# 불용어 제거 후 훈련 데이터와 테스트 데이터 분리\n",
    "X_train_add_nowords, X_test_add_nowords, y_train_add_nowords,y_test_add_nowords = train_test_split(text_data_add_nowords1, df1['code'], random_state = 1)\n",
    "\n",
    "\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "X_train_add_nowords_counts = count_vect.fit_transform(X_train_add_nowords)\n",
    "\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "X_train_add_nowords_tfidf = tfidf_transformer.fit_transform(X_train_add_nowords_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "clf_add_nowords= MultinomialNB().fit(X_train_add_nowords_tfidf, y_train_add_nowords)\n",
    "\n",
    "\n",
    "### 텍스트를 입력하면 TF-IDF 벡터로 바꾸는 전처리 함수\n",
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf\n",
    "\n",
    "\n",
    "\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "y_pred_add_nowords = clf_add_nowords.predict(tfidf_vectorizer(X_test_add_nowords))\n",
    "print(metrics.classification_report(y_test_add_nowords, y_pred_add_nowords))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 불용어 추가전 & 추가 후,  큰 차이가 없다...\n",
    "\n",
    " - 추가한 불용어가 학습하고 분류하는데에 큰 영향을 주지 않는것으로 생각된다.\n",
    " - 단어별로 몇 번 나왔는지 count 한 후, 불용어로 추가해 봐야겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 데이터 취합 결과\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "news 1 = IT/과학 \n",
    "         사회    \n",
    "         생활/문화\n",
    "         세 카테고리 모두 903개씩\n",
    "         \n",
    "news 2 = IT/과학  \n",
    "         사회    \n",
    "         생활/문화         \n",
    "         경제\n",
    "         네 카테고리 모두 235개씩\n",
    "         \n",
    "         \n",
    "         데이터 취합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번 데이터\n",
      "     code  count\n",
      "0  IT/과학    903\n",
      "1     사회   1668\n",
      "2  생활/문화   1423\n",
      "2번 데이터\n",
      "     code  count\n",
      "0  IT/과학    235\n",
      "1     경제    902\n",
      "2     사회    554\n",
      "3  생활/문화    446\n"
     ]
    }
   ],
   "source": [
    "print(\"1번 데이터\\n\",df1.groupby('code').size().reset_index(name = 'count'))\n",
    "print(\"2번 데이터\\n\",df2.groupby('code').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_eco=df2[:235]\n",
    "df2_social=df2[903:903+235]\n",
    "df2_cul=df2[1600:1600+235]\n",
    "df2_IT=df2[2000:2235]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_social=df1[:903]\n",
    "df1_cul=df1[1668:1668+903]\n",
    "df1_IT=df1[1668+1423:1668+1423+903]\n",
    "\n",
    "\n",
    "df1_sum=pd.concat([df1_social,df1_cul,df1_IT],ignore_index=True)\n",
    "df2_sum=pd.concat([df2_social,df2_cul,df2_IT,df2_eco],ignore_index=True)\n",
    "\n",
    "df_total=pd.concat([df1_sum,df2_sum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['에','는','은','을','했','에게','있','이','의','하','한','다','과',\n",
    "             '때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가',\n",
    "             '등','들','파이낸셜','저작','등','뉴스','배포', '매일신문', '으로', '에서', '이나', '전경', '사진',\n",
    "             '데', '다고', '도', '로', '뿐', '만', '해', '와', '겠', '달리', '면서', '를', '며', '라는','입니다',\n",
    "             '이미', '면서']\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "    temp_data = []\n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer.morphs(sentence) \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords_2] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data\n",
    "\n",
    "\n",
    "#불용어 추가전 데이터\n",
    "text_data_toal=preprocessing(df_total['news'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    code  count\n",
      "0  IT/과학   1040\n",
      "1     경제    235\n",
      "2     사회   1138\n",
      "3  생활/문화   1138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 49373 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 54876 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 47928 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 54868 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 49324 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 54924 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 44284 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 54617 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 44221 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 51228 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 49373 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 54876 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 47928 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 54868 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 49324 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 54924 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 44284 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 54617 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 44221 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 51228 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_total['code'].value_counts().plot(kind = 'bar')\n",
    "print(df_total.groupby('code').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "#- 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data_toal, df_total['code'], random_state = 1)\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.74      0.92      0.82       263\n",
      "          경제       1.00      0.21      0.34        48\n",
      "          사회       0.88      0.82      0.85       293\n",
      "       생활/문화       0.79      0.78      0.79       284\n",
      "\n",
      "    accuracy                           0.80       888\n",
      "   macro avg       0.85      0.68      0.70       888\n",
      "weighted avg       0.82      0.80      0.79       888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 텍스트를 입력하면 TF-IDF 벡터로 바꾸는 전처리 함수\n",
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf\n",
    "\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 취합 결과, 성능이 더 떨어졋다.\n",
    "\n",
    " - 경제 : 해당 카테고리의 기사 갯수가 적었는데, 그래서 Recall이 작게 나왔다.\n",
    " \n",
    " - 전체적인 성능은 줄어들었는데, 그 원인으로는\n",
    "\n",
    "1. 같은 날짜에 같은 주제의 기사들이 많이 올라오는데, 다른 날짜와 섞이면서, 주제가 늘어나고 분류하기가 어려워짐\n",
    "\n",
    "2. 데이터가 늘어난건 맞지만, 그만큼 주제도 늘어나면서 데이터가 분산(?)되며 더 정확도가 떨어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
