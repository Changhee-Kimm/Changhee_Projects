{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 구성\n",
    "\n",
    "#### 뇌는 1000억개의 신경계 뉴련이 있는데, 이를 본따서 만든 기술이 Neural Network\n",
    "\n",
    " - 가장 간단한 예로 우리는 MNIST 를 해보았다.\n",
    " - 짧고 간단한 코드로 무려 96%대의 정확도를 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 863us/step - loss: 0.4955 - accuracy: 0.8836\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 828us/step - loss: 0.2331 - accuracy: 0.9347\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 886us/step - loss: 0.1829 - accuracy: 0.9475\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 856us/step - loss: 0.1524 - accuracy: 0.9564\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 850us/step - loss: 0.1321 - accuracy: 0.9624\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 843us/step - loss: 0.1163 - accuracy: 0.9667\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 821us/step - loss: 0.1047 - accuracy: 0.9701\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 917us/step - loss: 0.0942 - accuracy: 0.9732\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 853us/step - loss: 0.0856 - accuracy: 0.9762\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 810us/step - loss: 0.0787 - accuracy: 0.9778\n",
      "313/313 - 0s - loss: 0.1133 - accuracy: 0.9673\n",
      "test_loss: 0.11325643211603165 \n",
      "test_accuracy: 0.9672999978065491\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 딥러닝 모델 구성 - 2 Layer Perceptron\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "\n",
    "# 모델 구성과 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](deepim1.png)\n",
    "\n",
    "총 3개의 레이어\n",
    "위의 코드에서는    \n",
    "입력층 784   \n",
    "은닝층 50   \n",
    "출력층 10   \n",
    "\n",
    " - 그림상으로는 3개의 레이어를 가졋다 생각할 수 있지만, 실제로는 총 2개의 레이어를 가졌다고 말한다.\n",
    " \n",
    " - 이런식으로 여러 층을 쌓아서 만든 것을 다층 퍼셉트론(Multi-Layer Perceptron:MLP) 라고 부르고 Deep Learning 은 층이 엄청 많은걸 뜻한다.\n",
    " \n",
    " ## 1. 이제 layer와 layer사이에 행렬 곱을 통해 Paramter, Weight를 찾아가게 된다.   \n",
    "    \n",
    " \n",
    " - Y= W * X + b\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(5, 784)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다.\n",
    "X = x_train_reshaped[:5]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "weight_init_std = 0.1\n",
    "input_size = 784\n",
    "hidden_size=50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)  \n",
    "# 바이어스 파라미터 b를 생성하고 Zero로 초기화\n",
    "b1 = np.zeros(hidden_size)\n",
    "\n",
    "a1 = np.dot(X, W1) + b1   # 은닉층 출력\n",
    "\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.43542854e+00, -4.80080738e-01, -4.00446556e-01,  3.21631359e-01,\n",
       "       -2.16162660e-01,  2.29409185e+00, -1.60624488e+00, -4.47405089e-01,\n",
       "        6.82677611e-01,  1.19854548e+00, -1.95362795e-01,  9.12329393e-01,\n",
       "       -2.13062536e-01, -1.64386896e+00, -9.76257363e-01,  1.08150938e+00,\n",
       "       -3.65113126e-01, -6.08839119e-01, -1.06071386e+00, -1.06025642e+00,\n",
       "        9.85056836e-01, -9.98985232e-01, -3.06294221e-01, -1.90112114e+00,\n",
       "        1.62158667e-01, -2.27570689e-01,  7.25191648e-01, -8.76437668e-01,\n",
       "        3.81248369e-01, -6.18084968e-01, -5.42723391e-01,  8.68443959e-01,\n",
       "        1.21253183e+00,  7.09551214e-01, -2.63733416e+00, -6.33068868e-01,\n",
       "       -1.83890350e-01, -1.12227191e+00,  1.78532608e+00,  3.69712403e-01,\n",
       "       -1.73613785e+00,  2.15135247e+00, -1.06281370e+00,  4.96640103e-01,\n",
       "       -1.69532169e+00,  5.35002316e-01, -2.51519612e-03, -1.40587833e+00,\n",
       "       -3.88479583e-01, -9.76114238e-01])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 데이터의 은닉층 출력을 확인해 봅시다.  50dim의 벡터가 나오나요?\n",
    "a1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 활성화 함수(Activation Function)\n",
    "\n",
    " - 각 노드 별로 입력값을 계산하고 다음 노드로 전달할 때 활성화 함수를 사용함.\n",
    " \n",
    " - 활성화 함수는 비선형인데, 선형으로 되면 결국 다층 Layer의 의미가 사라짐.\n",
    " - 비선형 활성화 함수는 더 많은 표현을 만들게 해줌.\n",
    " \n",
    "### 2-1.Sigmoid\n",
    "\n",
    " ![image](sigim1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80774574 0.38223306 0.40120505 0.57972178 0.44616878 0.90838655\n",
      " 0.16711062 0.38997791 0.66433605 0.76826593 0.45131405 0.71347659\n",
      " 0.44693496 0.1619393  0.27363504 0.74677951 0.40972239 0.35232406\n",
      " 0.25717306 0.25726046 0.72811045 0.26914098 0.42401953 0.12998164\n",
      " 0.54045107 0.44335159 0.67374923 0.29391653 0.59417416 0.35021712\n",
      " 0.36755428 0.70442182 0.77074662 0.67030199 0.06677397 0.34681501\n",
      " 0.45415653 0.24559011 0.85635328 0.59138948 0.14980416 0.89579509\n",
      " 0.25677212 0.62166942 0.15507727 0.63064907 0.4993712  0.19688497\n",
      " 0.40408336 0.27366348]\n"
     ]
    }
   ],
   "source": [
    "# 위 수식의 sigmoid 함수를 구현해 봅니다.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  \n",
    "\n",
    "\n",
    "z1 = sigmoid(a1)\n",
    "print(z1[0])  # sigmoid의 출력은 모든 엘리먼트가 0에서 1사이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 하지만 Sigmoid 함수는 잘 쓰이지 않는다.\n",
    " 1. Gradient Vanishing 현상이 발생 [참고]https://brunch.co.kr/@chris-song/39\n",
    " 2. Exp함수 계산량이 많다.\n",
    " 3. 함수의 중심 값이 0이 아니라 최적화가 느리다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tanh\n",
    "\n",
    "![im](tanhim1.png)\n",
    "\n",
    " -  sigmoid의 중심을 0으로 옮기긴 했지만,\n",
    " - 여전히 Vanishing Gradient 와 Exp계산량 문제가 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ReLU\n",
    "\n",
    "   #### F(x)= max(0,x)\n",
    "\n",
    "![im](Reluim1.png)\n",
    "\n",
    " - Sigmoid, tanh 보다 학습이 빠르고\n",
    " - 구현이 매우 간단하다.\n",
    " \n",
    " \n",
    " Sigmoid 다음에 다시 Dense Layer가 나오는데, 출력 노드 개수만 다르고 동일한 구조.\n",
    " 따라서 아래의 단일 레이어 로 함수를 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_layer_forward(X,W,b):\n",
    "    y=np.dot(X,W)+b\n",
    "    cache=(X,W,b)\n",
    "    return y,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09002884  0.35017235  0.33430221 -0.24940744 -0.15752454 -0.21052394\n",
      " -0.01117058 -0.07256756 -0.3491623   0.72618674]\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 50\n",
    "output_size = 10\n",
    "\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)    # z1이 다시 두번째 레이어의 입력이 됩니다. \n",
    "\n",
    "print(a2[0])  # 최종 출력이 output_size만큼의 벡터가 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09906334, 0.12849667, 0.1264735 , 0.07055011, 0.0773396 ,\n",
       "       0.07334737, 0.08952877, 0.08419732, 0.06385203, 0.1871513 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(a2)\n",
    "y_hat[0]  # 10개의 숫자 중 하나일 확률이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 손실 함수(Loss Function)\n",
    "\n",
    " - 정답과 예측값을 계산하는 함수.\n",
    " - MSE, RMSE, Cross Entropy, etc..\n",
    " [참고] https://towardsdatascience.com/understanding-different-loss-functions-for-neural-networks-dd1ed0274718\n",
    " \n",
    " #### Cross Entropy를 사용하기로 한다\n",
    " \n",
    " ![im](entropyim1.png)\n",
    " - Entropy는 두 확률 분포 사이의 유사도가 클수록 작아지는 값.\n",
    " - 앞선 y_hat[0]처럼 아직은 10프로 미만의 확률을 가진다.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 라벨을 One-hot 인코딩하는 함수\n",
    "def _change_ont_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "Y_digit = y_train[:5]\n",
    "t = _change_ont_hot_label(Y_digit, 10)\n",
    "t     # 정답 라벨의 One-hot 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정답 label과 Softmax(a2)의 분포가 유사하게 만드는게 목표!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09906334 0.12849667 0.1264735  0.07055011 0.0773396  0.07334737\n",
      " 0.08952877 0.08419732 0.06385203 0.1871513 ]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat[0])\n",
    "print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.24437349374822"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 경사하강법 Gradient Descent\n",
    " \n",
    " - Learning Rate : 발걸음 크기   \n",
    " [참고]https://medium.com/@peteryun/ml-%EB%AA%A8%EB%91%90%EB%A5%BC-%EC%9C%84%ED%95%9C-tensorflow-3-gradient-descent-algorithm-%EA%B8%B0%EB%B3%B8-c0688208fc59\n",
    " \n",
    " \n",
    " - Parameter : 초기화를 잘 해줘야 함. ==> 가중치 초기화   \n",
    " [참고]https://reniew.github.io/13/\n",
    " \n",
    " \n",
    " 아래는 경사를 구하는 미분 코딩.   \n",
    " [참고] https://deepnotes.io/softmax-crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01981267,  0.02569933,  0.0252947 ,  0.01411002,  0.01546792,\n",
       "        -0.18533053,  0.01790575,  0.01683946,  0.01277041,  0.03743026],\n",
       "       [-0.18232894,  0.02625696,  0.0228733 ,  0.01732723,  0.01508506,\n",
       "         0.01436715,  0.0150253 ,  0.01698824,  0.01441687,  0.03998883],\n",
       "       [ 0.01847345,  0.02372319,  0.02706664,  0.01459433, -0.18149517,\n",
       "         0.01381058,  0.01940078,  0.01537945,  0.01357221,  0.03547452],\n",
       "       [ 0.01818539, -0.17406371,  0.02673902,  0.01520168,  0.0139426 ,\n",
       "         0.01254382,  0.02219265,  0.01333014,  0.01329812,  0.03863029],\n",
       "       [ 0.02067052,  0.02267959,  0.02714208,  0.0165548 ,  0.01763776,\n",
       "         0.01348608,  0.018996  ,  0.01543509,  0.01298806, -0.16558999]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat - t) / batch_num\n",
    "dy    # softmax값의 출력으로 Loss를 미분한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dy = dLoss/dy  를 뜻하고, Chain Rule에 의해 다른 기울기들은 구하기 쉽다!    \n",
    "\n",
    "![im](chainim1.png)\n",
    "[참고]CS231n 강의의 BackPropagation에 자세히 나와있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06024755, -0.00698181,  0.05304858,  0.0322303 , -0.04808849,\n",
       "        -0.07736937,  0.0379548 ,  0.03289674,  0.02786758,  0.00868921],\n",
       "       [-0.02904074, -0.05777019,  0.06959747,  0.04102053, -0.08960695,\n",
       "        -0.08664334,  0.05091315,  0.04150412,  0.03582388,  0.02420208],\n",
       "       [-0.00414051, -0.02546956,  0.02881513,  0.01685552, -0.05563597,\n",
       "        -0.00692238,  0.02114498,  0.01665284,  0.01465209, -0.00595215],\n",
       "       [-0.11504017, -0.02734344,  0.06512467,  0.04027881, -0.07395925,\n",
       "        -0.05278829,  0.04672927,  0.04049216,  0.03487239,  0.04163384],\n",
       "       [-0.10033011,  0.01213217,  0.06860104,  0.04211553, -0.09947682,\n",
       "        -0.05344239,  0.04863729,  0.04261006,  0.0363305 ,  0.00282272],\n",
       "       [-0.04881277, -0.06738379,  0.07270973,  0.04345392, -0.06156806,\n",
       "        -0.08800173,  0.05315372,  0.0436353 ,  0.03766252,  0.01515115],\n",
       "       [ 0.00655224, -0.03517673,  0.06435359,  0.03760554, -0.10290039,\n",
       "        -0.0316497 ,  0.04687718,  0.03729662,  0.03238921, -0.05534754],\n",
       "       [-0.07859344, -0.05980888,  0.07917188,  0.04843737, -0.02755942,\n",
       "        -0.06077304,  0.05738317,  0.04773262,  0.04118687, -0.04717713],\n",
       "       [-0.05185191,  0.00784878,  0.07555553,  0.04525897, -0.08578341,\n",
       "        -0.11203697,  0.05393245,  0.04637458,  0.03911251, -0.01841053],\n",
       "       [-0.02511789, -0.05366029,  0.05620255,  0.03341922, -0.05514368,\n",
       "        -0.04965369,  0.04116057,  0.03323707,  0.02885354, -0.00929739],\n",
       "       [-0.03516369, -0.05881162,  0.05306919,  0.03195039, -0.02832414,\n",
       "        -0.04439985,  0.03889967,  0.03150728,  0.02738562, -0.01611285],\n",
       "       [-0.00194309, -0.01328548,  0.03454241,  0.02042028, -0.03397247,\n",
       "        -0.03133596,  0.02500761,  0.02036422,  0.0174549 , -0.03725241],\n",
       "       [-0.06696645, -0.07060376,  0.08676985,  0.05246054, -0.04824316,\n",
       "        -0.08174462,  0.06312683,  0.05206468,  0.04492078, -0.03178468],\n",
       "       [ 0.00078218,  0.00504257,  0.04097848,  0.02365952, -0.0936824 ,\n",
       "        -0.05303297,  0.0294647 ,  0.0244792 ,  0.02081726,  0.00149145],\n",
       "       [-0.09151665, -0.07384266,  0.08541982,  0.05222005, -0.06243919,\n",
       "        -0.03228786,  0.06208101,  0.05112364,  0.04455774, -0.03531589],\n",
       "       [-0.00153379, -0.04313495,  0.05005226,  0.02939995, -0.02935791,\n",
       "        -0.07962073,  0.03666434,  0.02967305,  0.02538449, -0.01752671],\n",
       "       [-0.04242571, -0.03595603,  0.0724118 ,  0.04312495, -0.06155756,\n",
       "        -0.12066531,  0.05245768,  0.04404383,  0.03744291,  0.01112344],\n",
       "       [-0.09158434, -0.03855183,  0.07468524,  0.04574672, -0.07978984,\n",
       "        -0.02841394,  0.05383123,  0.04517726,  0.03917588, -0.02027639],\n",
       "       [-0.11084235,  0.00376304,  0.06680008,  0.04161065, -0.05392926,\n",
       "        -0.05120996,  0.0473408 ,  0.04167534,  0.03549617, -0.0207045 ],\n",
       "       [-0.08252314, -0.05225155,  0.05166261,  0.03192676, -0.02207278,\n",
       "        -0.05220708,  0.03757334,  0.03178718,  0.027509  ,  0.02859567],\n",
       "       [-0.02140523, -0.05374037,  0.04944815,  0.02922665, -0.08604988,\n",
       "        -0.00765811,  0.03637353,  0.02871249,  0.02537494, -0.00028216],\n",
       "       [-0.04832211, -0.0554037 ,  0.08246213,  0.04960787, -0.04742111,\n",
       "        -0.08431943,  0.05986426,  0.04937257,  0.04242295, -0.04826343],\n",
       "       [-0.09583856, -0.04593028,  0.08324979,  0.05056244, -0.06086517,\n",
       "        -0.11922482,  0.06013806,  0.05128537,  0.04378546,  0.0328377 ],\n",
       "       [-0.09159472, -0.03350156,  0.08322833,  0.05045016, -0.06954009,\n",
       "        -0.12122471,  0.05994474,  0.05133275,  0.04370431,  0.0272008 ],\n",
       "       [-0.01422643, -0.06891082,  0.05809803,  0.03435899, -0.06097469,\n",
       "        -0.03309776,  0.04282089,  0.03376991,  0.0296007 , -0.02143883],\n",
       "       [-0.03667581,  0.008228  ,  0.07565924,  0.0450318 , -0.09971347,\n",
       "        -0.09763588,  0.05407825,  0.0459582 ,  0.03888868, -0.033819  ],\n",
       "       [-0.07520605, -0.02769115,  0.08212604,  0.04969755, -0.06375367,\n",
       "        -0.1062874 ,  0.05908981,  0.05024403,  0.04278691, -0.01100607],\n",
       "       [-0.04171071, -0.098633  ,  0.06787249,  0.04047301, -0.04690529,\n",
       "        -0.07154881,  0.05022603,  0.0401792 ,  0.03510389,  0.02494319],\n",
       "       [-0.05249577, -0.04267705,  0.06060581,  0.03682549, -0.01055196,\n",
       "        -0.0834382 ,  0.04393052,  0.03687305,  0.03148341, -0.0205553 ],\n",
       "       [-0.03151609,  0.01027696,  0.05293505,  0.03160668, -0.10262404,\n",
       "        -0.02337705,  0.03774791,  0.03175484,  0.02724227, -0.03404653],\n",
       "       [-0.05418994, -0.03606105,  0.04738923,  0.02867139, -0.05786674,\n",
       "        -0.04106957,  0.03442928,  0.02875421,  0.02491677,  0.02502642],\n",
       "       [-0.02888907, -0.01662434,  0.06074477,  0.03596487, -0.11091596,\n",
       "        -0.04052248,  0.0438539 ,  0.03618687,  0.03122651, -0.01102507],\n",
       "       [-0.04878213, -0.02578406,  0.05797391,  0.03470981, -0.09789697,\n",
       "        -0.03701947,  0.04191692,  0.03484028,  0.03016732,  0.00987438],\n",
       "       [-0.05055307, -0.05985613,  0.08642035,  0.05182182, -0.06757671,\n",
       "        -0.08534931,  0.06280938,  0.0516807 ,  0.0445041 , -0.03390113],\n",
       "       [ 0.00125754, -0.05749461,  0.04406147,  0.02568781, -0.05756965,\n",
       "        -0.02947623,  0.03265321,  0.02537169,  0.02229863, -0.00678987],\n",
       "       [-0.10289372, -0.01254256,  0.07392836,  0.04544457, -0.04923986,\n",
       "        -0.09955189,  0.05280767,  0.04609566,  0.03909336,  0.0068584 ],\n",
       "       [-0.08705578, -0.05360004,  0.07976769,  0.048673  , -0.03189888,\n",
       "        -0.09950281,  0.05772752,  0.04877968,  0.04176375, -0.00465413],\n",
       "       [-0.05661149, -0.01761447,  0.06715103,  0.04061794, -0.08264906,\n",
       "        -0.03682953,  0.04825113,  0.04041319,  0.03480493, -0.03753367],\n",
       "       [ 0.00161413, -0.03556368,  0.03436701,  0.02000432, -0.05004282,\n",
       "        -0.02639002,  0.0253245 ,  0.01990792,  0.01738689, -0.00660824],\n",
       "       [-0.02941254, -0.00205546,  0.05739939,  0.03389849, -0.10547492,\n",
       "        -0.06681749,  0.04122973,  0.03471852,  0.0296129 ,  0.00690139],\n",
       "       [-0.02760285, -0.0934136 ,  0.08370806,  0.049872  , -0.08671227,\n",
       "        -0.01703938,  0.06151635,  0.04847933,  0.0426506 , -0.06145824],\n",
       "       [-0.06717934, -0.08387673,  0.07869995,  0.04756944, -0.04620827,\n",
       "        -0.07163741,  0.05758219,  0.04712373,  0.04090006, -0.00297361],\n",
       "       [-0.06820895, -0.07270544,  0.0528942 ,  0.03216316, -0.05326372,\n",
       "        -0.03028605,  0.03892103,  0.03174187,  0.02793413,  0.04080978],\n",
       "       [-0.1175894 , -0.02037206,  0.09833123,  0.06004901, -0.06963669,\n",
       "        -0.13467042,  0.07039968,  0.06092068,  0.05168859,  0.00087937],\n",
       "       [-0.07446566, -0.0405127 ,  0.06921425,  0.04177868, -0.09911302,\n",
       "        -0.05167823,  0.05011427,  0.04191753,  0.03629647,  0.02644841],\n",
       "       [ 0.00533582, -0.0193806 ,  0.04617442,  0.02676265, -0.05135685,\n",
       "        -0.0919034 ,  0.03357636,  0.02765922,  0.02338782, -0.00025543],\n",
       "       [-0.08315559, -0.08395102,  0.07713819,  0.04672956, -0.08509912,\n",
       "        -0.03760099,  0.05644788,  0.04610225,  0.04040025,  0.02298858],\n",
       "       [-0.08629902, -0.05524212,  0.09937478,  0.05998225, -0.08893396,\n",
       "        -0.10425426,  0.07189983,  0.06018888,  0.05169161, -0.008408  ],\n",
       "       [-0.03160286, -0.06390557,  0.05903823,  0.03510777, -0.04984022,\n",
       "        -0.07027893,  0.04335432,  0.03516739,  0.03045686,  0.01250302],\n",
       "       [-0.00710351, -0.04043981,  0.07226636,  0.04250856, -0.07299164,\n",
       "        -0.09143139,  0.05257069,  0.04285606,  0.03669485, -0.03493017]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2 = np.dot(z1.T, dy)    \n",
    "dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2 = np.dot(z1.T, dy)\n",
    "db2 = np.sum(dy, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid 미분값\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "dz1 = np.dot(dy, W2.T)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dW1 = np.dot(X.T, da1)\n",
    "db1 = np.sum(dz1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파라미터 업데이트\n",
    "learning_rate = 0.1\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Backpropagation , 오역전파\n",
    "\n",
    " - 기울기를 입력층까지 전달해서 파라미터를 조정하는 것\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    " 1.  출력값과 예상값의 Loss를 구하고\n",
    " 2. Loss의 미분을 구해서\n",
    " 3. 각 레이어를 지나며\n",
    " 4. Weight를 업데이트 하는 것\n",
    " \n",
    " \n",
    " affine_layer_forward(X,W,b)에 대응해서 생각해보면!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_layer_backward(dy, cache):\n",
    "    X, W, b = cache\n",
    "    dX = np.dot(dy, W.T)\n",
    "    dW = np.dot(X.T, dy)\n",
    "    db = np.sum(dy, axis=0)\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05072021 0.14949809 0.03955676 0.17413785 0.08971604 0.11090491\n",
      "  0.10632165 0.05750313 0.12311204 0.09852933]\n",
      " [0.05849641 0.15335313 0.04127906 0.1738081  0.10692167 0.09286677\n",
      "  0.0931104  0.05897669 0.13640901 0.08477876]\n",
      " [0.06389198 0.13736734 0.04368228 0.13715011 0.10888195 0.10438668\n",
      "  0.10936593 0.07007994 0.14059849 0.08459531]\n",
      " [0.05168909 0.16360103 0.03944772 0.13570062 0.10792231 0.10347474\n",
      "  0.10887595 0.06594447 0.14447781 0.07886626]\n",
      " [0.05874323 0.1602994  0.04027166 0.15619145 0.10427799 0.09325002\n",
      "  0.12303113 0.05620549 0.11124337 0.09648626]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.280808438845478\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "# Forward Propagation\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "\n",
    "# 추론과 오차(Loss) 계산\n",
    "y_hat = softmax(a2)\n",
    "t = _change_ont_hot_label(Y_digit, 10)   # 정답 One-hot 인코딩\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "print(y_hat)\n",
    "print(t)\n",
    "print('Loss: ', Loss)\n",
    "        \n",
    "dy = (y_hat - t) / X.shape[0]\n",
    "dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "\n",
    "# 경사하강법을 통한 파라미터 업데이트    \n",
    "learning_rate = 0.1\n",
    "W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model  Training\n",
    "\n",
    " - Forward Propagation과 Back Propagation을 통해 W1,b1,W2,b2가 업데이트 되는 과정을 확인했다.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):\n",
    "    a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "    z1 = sigmoid(a1)\n",
    "    a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "    y_hat = softmax(a2)\n",
    "    t = _change_ont_hot_label(Y, 10)\n",
    "    Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "    if verbose:\n",
    "        print('---------')\n",
    "        print(y_hat)\n",
    "        print(t)\n",
    "        print('Loss: ', Loss)\n",
    "        \n",
    "    dy = (y_hat - t) / X.shape[0]\n",
    "    dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "    da1 = sigmoid_grad(a1) * dz1\n",
    "    dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "    \n",
    "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    \n",
    "    return W1, b1, W2, b2, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "[[0.11407058 0.15440484 0.12934466 0.18386625 0.08437038 0.04247947\n",
      "  0.08015739 0.07444332 0.06842433 0.06843877]\n",
      " [0.11023082 0.18018945 0.14429971 0.138643   0.07922013 0.03925765\n",
      "  0.09528661 0.07545858 0.0628828  0.07453124]\n",
      " [0.12717014 0.15252993 0.13087363 0.17442141 0.08855635 0.03957467\n",
      "  0.08997504 0.06768216 0.0614332  0.06778347]\n",
      " [0.1084934  0.13877369 0.14306293 0.15128846 0.09252734 0.04928251\n",
      "  0.09479998 0.07208178 0.0697961  0.07989381]\n",
      " [0.12299312 0.16146896 0.13012803 0.13457587 0.09453984 0.04318591\n",
      "  0.09578208 0.07386033 0.06716772 0.07629813]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.4672093849976866\n",
      "---------\n",
      "[[0.13364575 0.1629856  0.10809124 0.1477515  0.10219509 0.05819233\n",
      "  0.07080178 0.06806296 0.06364631 0.08462745]\n",
      " [0.13364427 0.18693954 0.11943544 0.11037957 0.09609859 0.05140288\n",
      "  0.08401551 0.06829235 0.05770592 0.09208593]\n",
      " [0.14731999 0.15931294 0.10848661 0.14016118 0.11182453 0.05086659\n",
      "  0.0796912  0.061495   0.05646668 0.08437529]\n",
      " [0.12279039 0.15229041 0.1199791  0.12272757 0.10879333 0.06206891\n",
      "  0.0837815  0.06618322 0.06514521 0.09624035]\n",
      " [0.14112926 0.16756203 0.10677471 0.10613423 0.11380697 0.05547676\n",
      "  0.08345697 0.06648539 0.06164302 0.09753065]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.2513908870571915\n",
      "---------\n",
      "[[0.14807323 0.16569839 0.09132432 0.12169214 0.11697004 0.07570296\n",
      "  0.06207978 0.06126588 0.0580913  0.09910197]\n",
      " [0.15337727 0.18731712 0.10008161 0.09015378 0.1104237  0.0641206\n",
      "  0.07354947 0.06093172 0.05202735 0.10801738]\n",
      " [0.16152266 0.160469   0.09092719 0.11532854 0.13373142 0.06231726\n",
      "  0.07001116 0.05504512 0.05097207 0.09967559]\n",
      " [0.13259824 0.1619488  0.10185948 0.1019507  0.12204761 0.07506144\n",
      "  0.07367625 0.0600747  0.05992804 0.11085474]\n",
      " [0.15359902 0.16805083 0.08885775 0.08597765 0.12992827 0.06801801\n",
      "  0.07233335 0.05907643 0.05565184 0.11850685]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.0841948012661633\n",
      "---------\n",
      "[[0.15804059 0.16530861 0.07805287 0.10217534 0.12848134 0.0946394\n",
      "  0.05445058 0.05483436 0.05258777 0.11142915]\n",
      " [0.16961755 0.18452899 0.08490101 0.07509556 0.12192457 0.07702901\n",
      "  0.06439882 0.05410279 0.04656091 0.12184079]\n",
      " [0.17070795 0.15875783 0.07713948 0.09672328 0.15369574 0.0735618\n",
      "  0.06150852 0.04902857 0.04567881 0.11319802]\n",
      " [0.13878212 0.16958375 0.08756681 0.08630073 0.13227885 0.0879477\n",
      "  0.06489909 0.05436539 0.05483591 0.12343963]\n",
      " [0.1613778  0.16579776 0.07498318 0.07111964 0.14267785 0.08042241\n",
      "  0.06280671 0.05229979 0.04992884 0.13858603]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.9510686455788044\n",
      "---------\n",
      "[[0.16457544 0.16348729 0.06743905 0.08714002 0.13693087 0.11451444\n",
      "  0.04794192 0.04904596 0.04748588 0.12143913]\n",
      " [0.18305344 0.18047168 0.07286233 0.06356513 0.13072991 0.08971075\n",
      "  0.05660244 0.0480507  0.04159797 0.13335564]\n",
      " [0.17615686 0.15584246 0.0662241  0.08243343 0.17158489 0.08422809\n",
      "  0.05425277 0.04368371 0.04087844 0.12471526]\n",
      " [0.1423299  0.17640665 0.07617258 0.07419794 0.13977346 0.10036983\n",
      "  0.05743798 0.04926119 0.05015346 0.13389702]\n",
      " [0.16571883 0.16248592 0.06409781 0.05984663 0.15227732 0.09228097\n",
      "  0.05481653 0.04635387 0.04476005 0.15736206]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.8423756242881404\n"
     ]
    }
   ],
   "source": [
    "X = x_train_reshaped[:5]\n",
    "Y = y_train[:5]\n",
    "\n",
    "# train_step을 다섯 번 반복 돌립니다.\n",
    "for i in range(5):\n",
    "    W1, b1, W2, b2, _ = train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 점점 Loss가 감소하는 것을 볼 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 추론 과정 구현과 정확도 계산\n",
    "\n",
    " - 앞에서 구한 W1,b1,W2,b2를 가지고 숫자를 예측하고, 정확도를 측정할 수 있다.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16867609, 0.16114441, 0.05885265, 0.07529905, 0.14276119,\n",
       "       0.13477438, 0.0424402 , 0.04395695, 0.0429057 , 0.12918937])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(W1, b1, W2, b2, X):\n",
    "    a1 = np.dot(X, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    y = softmax(a2)\n",
    "\n",
    "    return y\n",
    "\n",
    "# X = x_train[:100] 에 대해 모델 추론을 시도합니다. \n",
    "\n",
    "X = x_train_reshaped[:100]\n",
    "Y = y_test[:100]\n",
    "result = predict(W1, b1, W2, b2, X)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16867609 0.16114441 0.05885265 0.07529905 0.14276119 0.13477438\n",
      " 0.0424402  0.04395695 0.0429057  0.12918937]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "0.11\n"
     ]
    }
   ],
   "source": [
    "def accuracy(W1, b1, W2, b2, x, y):\n",
    "    y_hat = predict(W1, b1, W2, b2, x)\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "   # t = np.argmax(t, axis=1)\n",
    "\n",
    "    accuracy = np.sum(y_hat == y) / float(x.shape[0])\n",
    "    return accuracy\n",
    "\n",
    "acc = accuracy(W1, b1, W2, b2, X, Y)\n",
    "\n",
    "t = _change_ont_hot_label(Y, 10)\n",
    "print(result[0])\n",
    "print(t[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(50, 10)\n",
      "(10,)\n",
      "Loss:  2.304666900667875\n",
      "train acc, test acc | 0.10441666666666667, 0.1028\n",
      "Loss:  0.8530959048865384\n",
      "train acc, test acc | 0.7829166666666667, 0.7898\n",
      "Loss:  0.39670796440900924\n",
      "train acc, test acc | 0.8766833333333334, 0.8802\n",
      "Loss:  0.32082526119589894\n",
      "train acc, test acc | 0.8981333333333333, 0.9018\n",
      "Loss:  0.37550427443774603\n",
      "train acc, test acc | 0.9083833333333333, 0.9102\n",
      "Loss:  0.5409558126817127\n",
      "train acc, test acc | 0.9143833333333333, 0.9174\n",
      "Loss:  0.2503732387458357\n",
      "train acc, test acc | 0.9188166666666666, 0.9197\n",
      "Loss:  0.37231742894714165\n",
      "train acc, test acc | 0.9225, 0.926\n",
      "Loss:  0.2830105479104991\n",
      "train acc, test acc | 0.9272666666666667, 0.9293\n",
      "Loss:  0.18258983425939518\n",
      "train acc, test acc | 0.9309166666666666, 0.932\n",
      "Loss:  0.260547992637787\n",
      "train acc, test acc | 0.9337, 0.9335\n",
      "Loss:  0.17090092966607373\n",
      "train acc, test acc | 0.9364, 0.9373\n",
      "Loss:  0.170950960044137\n",
      "train acc, test acc | 0.9390166666666667, 0.9383\n",
      "Loss:  0.14353320967619942\n",
      "train acc, test acc | 0.9402833333333334, 0.9401\n",
      "Loss:  0.26106902136976656\n",
      "train acc, test acc | 0.9426666666666667, 0.9411\n",
      "Loss:  0.18336288797460543\n",
      "train acc, test acc | 0.94425, 0.9427\n",
      "Loss:  0.2222421912371149\n",
      "train acc, test acc | 0.9464, 0.9434\n",
      "Loss:  0.20025692873689532\n",
      "train acc, test acc | 0.9475, 0.9462\n",
      "Loss:  0.234967067184809\n",
      "train acc, test acc | 0.9489166666666666, 0.9463\n",
      "Loss:  0.11155636602037271\n",
      "train acc, test acc | 0.9509833333333333, 0.9475\n",
      "Loss:  0.24018689824724127\n",
      "train acc, test acc | 0.9521833333333334, 0.949\n",
      "Loss:  0.10958375053743578\n",
      "train acc, test acc | 0.9532, 0.9494\n",
      "Loss:  0.15823935414209603\n",
      "train acc, test acc | 0.95485, 0.9496\n",
      "Loss:  0.22685483899381617\n",
      "train acc, test acc | 0.9560166666666666, 0.9502\n",
      "Loss:  0.22225439248751983\n",
      "train acc, test acc | 0.9569666666666666, 0.9529\n",
      "Loss:  0.062120620960689904\n",
      "train acc, test acc | 0.9583333333333334, 0.9528\n",
      "Loss:  0.11276949817152436\n",
      "train acc, test acc | 0.9590166666666666, 0.9536\n",
      "Loss:  0.13161900314658023\n",
      "train acc, test acc | 0.9601333333333333, 0.9552\n",
      "Loss:  0.18619409007389268\n",
      "train acc, test acc | 0.9611833333333333, 0.9563\n",
      "Loss:  0.11634157454524308\n",
      "train acc, test acc | 0.9615166666666667, 0.9561\n",
      "Loss:  0.1791319518063527\n",
      "train acc, test acc | 0.9624, 0.9579\n",
      "Loss:  0.24046915970246185\n",
      "train acc, test acc | 0.9635, 0.9579\n",
      "Loss:  0.1599353954819267\n",
      "train acc, test acc | 0.9640333333333333, 0.9577\n",
      "Loss:  0.099466840511117\n",
      "train acc, test acc | 0.96485, 0.9596\n",
      "Loss:  0.0902286647224178\n",
      "train acc, test acc | 0.9655, 0.9597\n",
      "Loss:  0.15569602128043672\n",
      "train acc, test acc | 0.9663666666666667, 0.9602\n",
      "Loss:  0.07151469349140988\n",
      "train acc, test acc | 0.96675, 0.9609\n",
      "Loss:  0.10004985666532352\n",
      "train acc, test acc | 0.9677333333333333, 0.9606\n",
      "Loss:  0.062442316703621464\n",
      "train acc, test acc | 0.9679166666666666, 0.9614\n",
      "Loss:  0.15369316278435907\n",
      "train acc, test acc | 0.9686, 0.9607\n",
      "Loss:  0.07493420625862215\n",
      "train acc, test acc | 0.9688166666666667, 0.9611\n",
      "Loss:  0.07897644625414997\n",
      "train acc, test acc | 0.9697833333333333, 0.9615\n",
      "Loss:  0.056520308344207376\n",
      "train acc, test acc | 0.9704166666666667, 0.9632\n",
      "Loss:  0.17086393363613442\n",
      "train acc, test acc | 0.9706166666666667, 0.9634\n",
      "Loss:  0.15064339765963178\n",
      "train acc, test acc | 0.97125, 0.9639\n",
      "Loss:  0.11239858638868476\n",
      "train acc, test acc | 0.9717, 0.9642\n",
      "Loss:  0.06020413930517541\n",
      "train acc, test acc | 0.9722333333333333, 0.9649\n",
      "Loss:  0.03797560518143432\n",
      "train acc, test acc | 0.9726333333333333, 0.9642\n",
      "Loss:  0.12607013169855968\n",
      "train acc, test acc | 0.9728, 0.9648\n",
      "Loss:  0.0804293905088905\n",
      "train acc, test acc | 0.9735166666666667, 0.9653\n",
      "Loss:  0.08981148212392229\n",
      "train acc, test acc | 0.97385, 0.965\n",
      "Loss:  0.13244277278236397\n",
      "train acc, test acc | 0.9743166666666667, 0.9655\n",
      "Loss:  0.05316023783676272\n",
      "train acc, test acc | 0.9746666666666667, 0.9653\n",
      "Loss:  0.07349848069548325\n",
      "train acc, test acc | 0.9751166666666666, 0.9665\n",
      "Loss:  0.023805057112797637\n",
      "train acc, test acc | 0.97545, 0.9664\n",
      "Loss:  0.1280387865971286\n",
      "train acc, test acc | 0.97555, 0.9656\n",
      "Loss:  0.14259007418735828\n",
      "train acc, test acc | 0.9756666666666667, 0.9662\n",
      "Loss:  0.05109901396812735\n",
      "train acc, test acc | 0.9764666666666667, 0.9665\n",
      "Loss:  0.08922678171054868\n",
      "train acc, test acc | 0.9766166666666667, 0.9664\n",
      "Loss:  0.06519281834703386\n",
      "train acc, test acc | 0.9769666666666666, 0.967\n",
      "Loss:  0.04637057697482797\n",
      "train acc, test acc | 0.9771333333333333, 0.9672\n",
      "Loss:  0.12979991444122455\n",
      "train acc, test acc | 0.9778, 0.9667\n",
      "Loss:  0.09254513924455395\n",
      "train acc, test acc | 0.9780166666666666, 0.9672\n",
      "Loss:  0.13721163269872214\n",
      "train acc, test acc | 0.9780833333333333, 0.968\n",
      "Loss:  0.09009431936353036\n",
      "train acc, test acc | 0.9785, 0.9686\n",
      "Loss:  0.037729843948053635\n",
      "train acc, test acc | 0.9787333333333333, 0.968\n",
      "Loss:  0.04865207928219178\n",
      "train acc, test acc | 0.9792, 0.9672\n",
      "Loss:  0.1025808819338313\n",
      "train acc, test acc | 0.9793833333333334, 0.9682\n",
      "Loss:  0.0747525412020914\n",
      "train acc, test acc | 0.9798833333333333, 0.9692\n",
      "Loss:  0.10273808218587796\n",
      "train acc, test acc | 0.9800666666666666, 0.9679\n",
      "Loss:  0.07531261514287976\n",
      "train acc, test acc | 0.9804333333333334, 0.9685\n",
      "Loss:  0.06477625919203914\n",
      "train acc, test acc | 0.9803833333333334, 0.968\n",
      "Loss:  0.09047887110252308\n",
      "train acc, test acc | 0.98095, 0.9687\n",
      "Loss:  0.03410843484706568\n",
      "train acc, test acc | 0.9810166666666666, 0.9692\n",
      "Loss:  0.055895155456391975\n",
      "train acc, test acc | 0.9812166666666666, 0.9687\n",
      "Loss:  0.05108017727891033\n",
      "train acc, test acc | 0.98145, 0.9689\n",
      "Loss:  0.09349095479519126\n",
      "train acc, test acc | 0.9817333333333333, 0.9697\n",
      "Loss:  0.0487151410457666\n",
      "train acc, test acc | 0.9816833333333334, 0.9689\n",
      "Loss:  0.07322164038463597\n",
      "train acc, test acc | 0.9818166666666667, 0.9692\n",
      "Loss:  0.026438880894831148\n",
      "train acc, test acc | 0.9822333333333333, 0.9698\n",
      "Loss:  0.0429156701464342\n",
      "train acc, test acc | 0.9822666666666666, 0.9706\n",
      "Loss:  0.021531547726700587\n",
      "train acc, test acc | 0.9827, 0.9706\n",
      "Loss:  0.043919648103382024\n",
      "train acc, test acc | 0.9828666666666667, 0.9696\n",
      "Loss:  0.03175787978007905\n",
      "train acc, test acc | 0.9830666666666666, 0.9697\n"
     ]
    }
   ],
   "source": [
    "def init_params(input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "\n",
    "    W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "    b1 = np.zeros(hidden_size)\n",
    "    W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "    b2 = np.zeros(output_size)\n",
    "\n",
    "    print(W1.shape)\n",
    "    print(b1.shape)\n",
    "    print(W2.shape)\n",
    "    print(b2.shape)\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 50000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "W1, b1, W2, b2 = init_params(784, 50, 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train_reshaped[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    W1, b1, W2, b2, Loss = train_step(x_batch, y_batch, W1, b1, W2, b2, learning_rate=0.1, verbose=False)\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    train_loss_list.append(Loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        print('Loss: ', Loss)\n",
    "        train_acc = accuracy(W1, b1, W2, b2, x_train_reshaped, y_train)\n",
    "        test_acc = accuracy(W1, b1, W2, b2, x_test_reshaped, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 6 \n",
    "\n",
    "# Accuracy 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss 그래프 그리기\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, train_loss_list, label='train acc')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, 3.0)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
